var __index = Promise.resolve({"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":false},"docs":[{"location":"api/","text":"<p>Cirrus CI exposes GraphQL API for integrators to use through <code>https://api.cirrus-ci.com/graphql</code> endpoint. Please check Cirrus CI GraphQL Schema for a full list of  available types and methods. Or check built-in interactive GraphQL Explorer. Here is an example of how to get a build for a particular SHA of a given repository:</p> <pre><code>curl -X POST --data \\\n'{\n  \"query\": \"query BuildBySHAQuery($owner: String!, $name: String!, $SHA: String) { searchBuilds(repositoryOwner: $owner, repositoryName: $name, SHA: $SHA) { id } }\",\n  \"variables\": {\n    \"owner\": \"ORGANIZATION\",\n    \"name\": \"REPOSITORY NAME\",\n    \"SHA\": \"SOME SHA\"\n  }\n}' \\\nhttps://api.cirrus-ci.com/graphql | python -m json.tool\n</code></pre>","title":"Cirrus CI API"},{"location":"api/#authorization","text":"<p>In order for a tool to access Cirrus CI API, an organization admin should generate an access token through Cirrus CI Settings page for a corresponding organization. Here is a direct link to the settings page: <code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>. Access tokens will allow full write and read access to both public and private repositories of your organization on Cirrus CI: it will be possible to create new builds and perform any other GraphQL mutations. If you only need read access to public repositories of your organization you can skip this step and don't provide <code>Authorization</code> header.</p> <p>Once an access token is generated and securely stored, it can be used to authorize API requests by setting <code>Authorization</code> header to <code>Bearer $TOKEN</code>.</p>  <p>User API Token Permission Scope</p> <p>It is also possible to generate API tokens for personal accounts but they will be scoped only to access personal public and private repositories of a particular user. It won't be possible to access private repositories of an organization, even if they have access.</p>","title":"Authorization"},{"location":"api/#webhooks","text":"<p>It is possible to subscribe for updates of builds and tasks. If a WebHook URL is configured on Cirrus CI Settings page for  an organization, Cirrus CI will try to <code>POST</code> a webhook event payload to this URL.</p> <p><code>POST</code> request will contain <code>X-Cirrus-Event</code> header to specify if the update was made to a <code>build</code> or a <code>task</code>. The event  payload itself is pretty basic:</p> <pre><code>{\n  \"action\": \"created\" | \"updated\",\n  \"data\": ...\n}\n</code></pre> <p><code>data</code> field will be populated by executing the following GraphQL query:</p> <pre><code>repository(id: $repositoryId) {\n  id\n  owner\n  name\n  isPrivate\n}\nbuild(id: $buildId) {\n  id\n  branch\n  pullRequest\n  changeIdInRepo\n  changeTimestamp\n  status\n}\ntask(id: $taskId) {\n  id\n  name\n  status\n  statusTimestamp\n  creationTimestamp\n  uniqueLabels\n  automaticReRun\n  automaticallyReRunnable\n}\n</code></pre>  <p>Custom GraphQL Query</p> <p>If you'd like to customize GraphQL query which will be executed and included in the event payload please contact support for further details.</p>","title":"WebHooks"},{"location":"api/#securing-webhooks","text":"<p>Imagine you've been given a <code>https://example.com/webhook</code> endpoint by your administrator, and for some reason there's no easy way to change that. This kind of URL is easily discoverable on the internet, and an attacker can take advantage of this by sending requests to this URL, thus pretending to be the Cirrus CI.</p> <p>To avoid such situations, set the secret token in the repository settings, and then validate the <code>X-Cirrus-Signature</code> for each WebHook request.</p> <p>Once configured, the secret token and the request's body are fed into the HMAC algorithm to generate the <code>X-Cirrus-Signature</code> for each request coming from the Cirrus CI.</p>  <p>Missing X-Cirrus-Signature header</p> <p>When secret token is configured in the repository settings, all WebHook requests will contain the <code>X-Cirrus-Signature-Header</code>. Make sure to assert the presence of <code>X-Cirrus-Signature-Header</code> header and correctness of its value in your validation code.</p>  <p>Using HMAC is pretty straightforward in many languages, here's an example of how to validate the <code>X-Cirrus-Signature</code> using Python's <code>hmac</code> module:</p> <pre><code>import hmac\n\ndef is_signature_valid(secret_token: bytes, body: bytes, x_cirrus_signature: str) -&gt; bool:\n    expected_signature = hmac.new(secret_token, body, \"sha256\").hexdigest()\n\n    return hmac.compare_digest(expected_signature, x_cirrus_signature)\n</code></pre>","title":"Securing WebHooks"},{"location":"examples/","text":"<p>Here you can find example configurations per different programming languages/frameworks.</p>","title":"Examples"},{"location":"examples/#android","text":"<p>Cirrus CI has a set of Docker images ready for Android development.  If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI. For those images <code>.cirrus.yml</code> configuration file can look like:</p> <pre><code>container:\n  image: cirrusci/android-sdk:29\n\ncheck_android_task:\n  check_script: ./gradlew check connectedCheck\n</code></pre> <p>Or like this if a running emulator is needed for the tests:</p> <pre><code>container:\n  image: cirrusci/android-sdk:29\n  cpu: 4\n  memory: 10G\n\ncheck_android_task:\n  create_device_script:\n    echo no | avdmanager create avd --force\n        -n test\n        -k \"system-images;android-29;default;armeabi-v7a\"\n  start_emulator_background_script:\n    $ANDROID_HOME/emulator/emulator\n        -avd test\n        -no-audio\n        -no-window\n  wait_for_emulator_script:\n    - adb wait-for-device\n    - adb shell input keyevent 82\n  check_script: ./gradlew check connectedCheck\n</code></pre>  <p>Info</p> <p>Please don't forget to setup Remote Build Cache for your Gradle project. Or at least basic folder caching.</p>","title":"Android"},{"location":"examples/#android-lint","text":"<p>The Cirrus CI annotator supports providing inline reports on PRs and can parse Android Lint reports. Here is an example of an Android Lint task that you can add to your <code>.cirrus.yml</code>:</p> <pre><code>task:\n  name: Android Lint\n  lint_script: ./gradlew lintDebug\n  always:\n    android-lint_artifacts:\n      path: \"**/reports/lint-results-debug.xml\"\n      type: text/xml\n      format: android-lint\n</code></pre>","title":"Android Lint"},{"location":"examples/#bazel","text":"<p>Bazel Team provides a set of official Docker images with Bazel pre-installed. Here is an example of how <code>.cirrus.yml</code> can look like for Bazel:</p> amd64arm64   <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre>   <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre>    <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>","title":"Bazel"},{"location":"examples/#remote-cache","text":"<p>Cirrus CI has built-in HTTP Cache which is compatible with Bazel's remote cache.</p> <p>Here is an example of how Cirrus CI HTTP Cache can be used with Bazel:</p> amd64arm64   <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre>   <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre>","title":"Remote Cache"},{"location":"examples/#c","text":"<p>Official GCC Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre>   <pre><code>arm_container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre>","title":"C++"},{"location":"examples/#crystal","text":"<p>Official Crystal Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches dependencies and runs tests:</p> <pre><code>container:\n  image: crystallang/crystal:latest\n\nspec_task:\n  shard_cache:\n    fingerprint_script: cat shard.lock\n    populate_script: shards install\n    folder: lib\n  spec_script: crystal spec\n</code></pre>","title":"Crystal"},{"location":"examples/#elixir","text":"<p>Official Elixir Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre>","title":"Elixir"},{"location":"examples/#erlang","text":"<p>Official Erlang Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre>","title":"Erlang"},{"location":"examples/#flutter","text":"<p>Cirrus CI provides a set of Docker images with Flutter and Dart SDK pre-installed. Here is an example of how <code>.cirrus.yml</code> can be written for Flutter:</p> <pre><code>container:\n  image: cirrusci/flutter:latest\n\ntest_task:\n  pub_cache:\n    folder: ~/.pub-cache\n  test_script: flutter test -machine &gt; report.json\n  always:\n    report_artifacts:\n      path: report.json\n      format: flutter\n</code></pre> <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>","title":"Flutter"},{"location":"examples/#flutter-web","text":"<p>Our Docker images with Flutter and Dart SDK pre-installed have special <code>*-web</code> tags with Chromium pre-installed. You can use these tags to run Flutter Web </p> <p>First define a new <code>chromium</code> platform in your <code>dart_test.yaml</code>:</p> <pre><code>define_platforms:\n  chromium:\n    name: Chromium\n    extends: chrome\n    settings:\n      arguments: --no-sandbox\n      executable:\n        linux: chromium\n</code></pre> <p>Now you'll be able to run tests targeting web via <code>pub run test test -p chromium</code></p>","title":"Flutter Web"},{"location":"examples/#go","text":"<p>The best way to test Go projects is by using official Go Docker images. Here is an example of how <code>.cirrus.yml</code> can look like for a project using Go Modules:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre>","title":"Go"},{"location":"examples/#golangci-lint","text":"<p>We highly recommend to configure some sort of linting for your Go project. One of the options is GolangCI Lint. The Cirrus CI annotator supports providing inline reports on PRs and can parse GolangCI Lint reports. Here is an example of a GolangCI Lint task that you can add to your <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>task:\n  name: GolangCI Lint\n  container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre>   <pre><code>task:\n  name: GolangCI Lint\n  arm_container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre>","title":"GolangCI Lint"},{"location":"examples/#gradle","text":"<p>We recommend use of the official Gradle Docker containers since they have Gradle specific configurations already set up. For example, standard Java containers don't have  a pre-configured user and as a result don't have <code>HOME</code> environment variable presented which makes Gradle complain.</p>","title":"Gradle"},{"location":"examples/#caching","text":"<p>To preserve caches between Gradle runs, add a cache instruction as shown below. The trick here is to clean up <code>~/.gradle/caches</code> folder in the very end of a build. Gradle creates some unique nondeterministic files in <code>~/.gradle/caches</code> folder on every run which makes Cirrus CI re-upload the cache every time. This way, you get faster builds!</p> amd64arm64   <pre><code>container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre>   <pre><code>arm_container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre>","title":"Caching"},{"location":"examples/#build-cache","text":"<p>Here is how HTTP Cache can be used with Gradle by adding the following code to <code>settings.gradle</code>:</p> <pre><code>ext.isCiServer = System.getenv().containsKey(\"CIRRUS_CI\")\next.isMasterBranch = System.getenv()[\"CIRRUS_BRANCH\"] == \"master\"\next.buildCacheHost = System.getenv().getOrDefault(\"CIRRUS_HTTP_CACHE_HOST\", \"localhost:12321\")\n\nbuildCache {\n  local {\n    enabled = !isCiServer\n  }\n  remote(HttpBuildCache) {\n    url = \"http://${buildCacheHost}/\"\n    enabled = isCiServer\n    push = isMasterBranch\n  }\n}\n</code></pre> <p>If your project uses a <code>buildSrc</code> directory, the build cache configuration should also be applied to <code>buildSrc/settings.gradle</code>.</p> <p>To do this, put the build cache configuration above into a separate <code>gradle/buildCacheSettings.gradle</code> file, then apply it to both your <code>settings.gradle</code> and <code>buildSrc/settings.gradle</code>.</p> <p>In <code>settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, 'gradle/buildCacheSettings.gradle')\n</code></pre> <p>In <code>buildSrc/settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, '../gradle/buildCacheSettings.gradle')\n</code></pre> <p>Please make sure you are running Gradle commands with <code>--build-cache</code> flag or have <code>org.gradle.caching</code> enabled in <code>gradle.properties</code> file. Here is an example of a <code>gradle.properties</code> file that we use internally for all Gradle projects:</p> <pre><code>org.gradle.daemon=true\norg.gradle.caching=true\norg.gradle.parallel=true\norg.gradle.configureondemand=true\norg.gradle.jvmargs=-Dfile.encoding=UTF-8\n</code></pre>","title":"Build Cache"},{"location":"examples/#junit","text":"<p>Here is a <code>.cirrus.yml</code> that, parses and uploads JUnit reports at the end of the build:</p> <pre><code>junit_test_task:\n  junit_script: &lt;replace this comment with instructions to run the test suites&gt;\n  always:\n    junit_result_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n      type: text/xml\n</code></pre> <p>If it is running on a pull request, annotations will also be displayed in-line.</p>","title":"JUnit"},{"location":"examples/#maven","text":"<p>Official Maven Docker images can be used for building and testing Maven projects:</p> amd64arm64   <pre><code>container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre>   <pre><code>arm_container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre>","title":"Maven"},{"location":"examples/#mysql","text":"<p>The Additional Containers feature makes it super simple to run the same Docker MySQL image as you might be running in production for your application. Getting a running instance of the latest GA  version of MySQL can used with the following six lines in your <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>    <p>With the configuration above MySQL will be available on <code>localhost:3306</code>. Use empty password to login as <code>root</code> user. </p>","title":"MySQL"},{"location":"examples/#node","text":"<p>Official NodeJS Docker images can be used for building and testing Node.JS applications.</p>","title":"Node"},{"location":"examples/#npm","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on contents of <code>package-lock.json</code> file and runs tests:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre>","title":"npm"},{"location":"examples/#yarn","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on the contents of a <code>yarn.lock</code> file and runs tests:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre>","title":"Yarn"},{"location":"examples/#yarn-2","text":"<p>Yarn 2 (also known as Yarn Berry), has a different package cache location (<code>.yarn/cache</code>). To run tests, it would look like this:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre>","title":"Yarn 2"},{"location":"examples/#eslint-annotations","text":"<p>ESLint reports are supported by Cirrus CI Annotations. This way you can see all the linting issues without leaving the pull request you are reviewing! You'll need to generate an <code>ESLint</code> report file (for example, <code>eslint.json</code>) in one of your task's scripts. Then save it as an artifact in <code>eslint</code> format:</p> <pre><code>task:\n  # boilerplate\n  eslint_script: ...\n  always:\n    eslint_report_artifact:\n      path: eslint.json\n      format: eslint\n</code></pre>","title":"ESLint Annotations"},{"location":"examples/#python","text":"<p>Official Python Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code>  that caches installed packages based on contents of <code>requirements.txt</code> and runs <code>pytest</code>:</p> amd64arm64   <pre><code>container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre>   <pre><code>arm_container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre>","title":"Python"},{"location":"examples/#building-pypi-packages","text":"<p>Also using the Python Docker images, you can run tests if you are making packages for PyPI. Here is an example <code>.cirrus.yml</code> for doing so:</p> amd64arm64   <pre><code>container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre>   <pre><code>arm_container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre>","title":"Building PyPI Packages"},{"location":"examples/#linting","text":"<p>You can easily set up linting with Cirrus CI and flake8, here is an example <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>lint_task:\n  container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre>   <pre><code>lint_task:\n  arm_container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre>","title":"Linting"},{"location":"examples/#unittest-annotations","text":"<p>Python Unittest reports are supported by Cirrus CI Annotations. This way you can see what tests are failing without leaving the pull request you are reviewing! Here is an example of a <code>.cirrus.yml</code> that produces and stores <code>Unittest</code> reports:</p> amd64arm64   <pre><code>unittest_task:\n  container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre>   <pre><code>unittest_task:\n  arm_container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre>    <p>Now you should get annotations for your test results.</p>","title":"<code>Unittest</code> Annotations"},{"location":"examples/#qodana","text":"<p>Qodana by JetBrains is a code quality monitoring tool that identifies and suggests fixes for bugs, security vulnerabilities, duplications, and imperfections. It brings all the smart features you love in the JetBrains IDEs.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save Qodana's report as an artifact, will parse it and report as annotations:</p> <pre><code>task:\n  name: Qodana\n  container:\n    image: jetbrains/qodana:latest\n  env:\n    CIRRUS_WORKING_DIR: /data/project\n  generate_report_script:\n    - /opt/idea/bin/entrypoint --save-report --report-dir=report\n  always:\n    results_artifacts:\n      path: \"report/results/result-allProblems.json\"\n      format: qodana\n</code></pre>","title":"Qodana"},{"location":"examples/#release-assets","text":"<p>Cirrus CI doesn't provide a built-in functionality to upload artifacts on a GitHub release but this functionality can be added via a script. For a release, Cirrus CI will provide <code>CIRRUS_RELEASE</code> environment variable along with <code>CIRRUS_TAG</code>  environment variable. <code>CIRRUS_RELEASE</code> indicates release id which can be used to upload assets.</p> <p>Cirrus CI only requires write access to Check API and doesn't require write access to repository contents because of security  reasons. That's why you need to create a personal access token with full access to <code>repo</code> scope. Once an access token is created, please create an encrypted variable  from it and save it to <code>.cirrus.yml</code>:</p> <pre><code>env:\n  GITHUB_TOKEN: ENCRYPTED[qwerty]\n</code></pre> <p>Now you can use a script to upload your assets:</p> <pre><code>#!/usr/bin/env bash\n\nif [[ \"$CIRRUS_RELEASE\" == \"\" ]]; then\n  echo \"Not a release. No need to deploy!\"\n  exit 0\nfi\n\nif [[ \"$GITHUB_TOKEN\" == \"\" ]]; then\n  echo \"Please provide GitHub access token via GITHUB_TOKEN environment variable!\"\n  exit 1\nfi\n\nfile_content_type=\"application/octet-stream\"\nfiles_to_upload=(\n  # relative paths of assets to upload\n)\n\nfor fpath in $files_to_upload\ndo\n  echo \"Uploading $fpath...\"\n  name=$(basename \"$fpath\")\n  url_to_upload=\"https://uploads.github.com/repos/$CIRRUS_REPO_FULL_NAME/releases/$CIRRUS_RELEASE/assets?name=$name\"\n  curl -X POST \\\n    --data-binary @$fpath \\\n    --header \"Authorization: token $GITHUB_TOKEN\" \\\n    --header \"Content-Type: $file_content_type\" \\\n    $url_to_upload\ndone\n</code></pre>","title":"Release Assets"},{"location":"examples/#ruby","text":"<p>Official Ruby Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches installed gems based on Ruby version, contents of <code>Gemfile.lock</code>, and runs <code>rspec</code>:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>     Repositories without <code>Gemfile.lock</code> <p>When you are not committing <code>Gemfile.lock</code> (in Ruby gems repositories, for example) you can run <code>bundle install</code> (or <code>bundle update</code>) in <code>install_script</code> instead of <code>populate_script</code> in <code>bundle_cache</code>. Cirrus Agent is clever enough to re-upload cache entry only if cached folder has been changed during task execution. Here is an example of a <code>.cirrus.yml</code> that always runs <code>bundle install</code>:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre>      <p>Test Parallelization</p> <p>It's super easy to add intelligent test splitting by using Knapsack Pro and matrix modification. After setting up Knapsack Pro gem, you can add sharding like this:</p> <pre><code>task:\n  matrix:\n    name: rspec (shard 1)\n    name: rspec (shard 2)\n    name: rspec (shard 3)\n    name: rspec (shard 4)\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script: cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rake knapsack_pro:rspec\n</code></pre> <p>Which will create four shards that will theoretically run tests 4x faster by equally splitting all tests between  these four shards.</p>","title":"Ruby"},{"location":"examples/#rspec-and-rubocop-annotations","text":"<p>Cirrus CI natively supports RSpec and RuboCop machine-parsable JSON reports.</p> <p>To get behavior-driven test annotations, generate and upload a <code>rspec</code> artifact from your lint task:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>    <p>Generate a <code>rubocop</code> artifact to quickly gain context for linter/formatter annotations:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre>","title":"RSpec and RuboCop Annotations"},{"location":"examples/#rust","text":"<p>Official Rust Docker images can be used for builds. Here is a basic example of <code>.cirrus.yml</code>  that caches crates in <code>$CARGO_HOME</code> based on contents of <code>Cargo.lock</code>:</p> amd64arm64   <pre><code>container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>   <pre><code>arm_container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>     <p>Caching Cleanup</p> <p>Please note <code>before_cache_script</code> that removes registry index from the cache before uploading it in the end of a successful task.  Registry index is changing very rapidly making the cache invalid. <code>before_cache_script</code> deletes the index and leaves only the required crates for caching.</p>","title":"Rust"},{"location":"examples/#rust-nightly","text":"<p>It is possible to use nightly builds of Rust via an official <code>rustlang/rust:nightly</code> container.  Here is an example of a <code>.cirrus.yml</code> to run tests against the latest stable and nightly versions of Rust:</p> amd64arm64   <pre><code>test_task:\n  matrix:\n    - container:\n        image: rust:latest\n    - allow_failures: true\n      container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>   <pre><code>test_task:\n  matrix:\n    - arm_container:\n        image: rust:latest\n    - allow_failures: true\n      arm_container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>     FreeBSD Caveats <p>Vanila FreeBSD VMs don't set some environment variables required by Cargo for effective caching. Specifying <code>HOME</code> environment variable to some arbitrarily location should fix caching:</p> <pre><code>freebsd_instance:\n  image-family: freebsd-12-0\n\ntask:\n  name: cargo test (stable)\n  env:\n    HOME: /tmp # cargo needs it\n  install_script: pkg install -y rust\n  cargo_cache:\n    folder: $HOME/.cargo/registry\n    fingerprint_script: cat Cargo.lock\n  build_script: cargo build --all\n  test_script: cargo test --all --all-targets\n  before_cache_script: rm -rf $HOME/.cargo/registry/index\n</code></pre>","title":"Rust Nightly"},{"location":"examples/#xclogparser","text":"<p>XCLogParser is a CLI tool that parses Xcode and <code>xcodebuild</code>'s logs (<code>xcactivitylog</code> files) and produces reports in different formats.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save XCLogParser's flat JSON report as an artifact, will parse it and report as annotations:</p> <pre><code>macos_instance:\n  image: big-sur-xcode\n\ntask:\n  name: XCLogParser\n  build_script:\n    - xcodebuild -scheme noapp -derivedDataPath ~/dd\n  always:\n    xclogparser_parse_script:\n      - brew install xclogparser\n      - xclogparser parse --project noapp --reporter flatJson --output xclogparser.json --derived_data ~/dd\n    xclogparser_upload_artifacts:\n      path: \"xclogparser.json\"\n      type: text/json\n      format: xclogparser\n</code></pre>","title":"XCLogParser"},{"location":"faq/","text":"","title":"Frequently Asked Questions"},{"location":"faq/#is-cirrus-ci-a-delivery-platform","text":"<p>Cirrus CI is not positioned as a delivery platform but can be used as one for many general use cases by having  Dependencies between tasks and using Conditional Task Execution or Manual Tasks:</p> <pre><code>lint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  only_if: $BRANCH == 'master'\n  trigger_type: manual\n  depends_on: \n    - test\n    - lint\n  script: yarn run publish\n</code></pre>","title":"Is Cirrus CI a delivery platform?"},{"location":"faq/#are-there-any-limits","text":"<p>Cirrus CI has the following limitations on how many CPUs for different platforms a single user can run on community clusters for public repositories for free:</p> <ul> <li>16.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Arm Linux platform (Containers).</li> <li>8.0 CPUs for Windows platform (Containers or VMs)</li> <li>8.0 CPUs for FreeBSD VMs.</li> <li>12.0 CPUs macOS VM (1 VM).</li> </ul> <p>Note that a single task can't request more than 8 CPUs (except macOS VMs which are not configurable).</p>  <p>No Monthly Minute Limit</p> <p>There are no limits on how many minutes a month you can use! Please keep in mind that mining cryptocurrency is against our Terms of Service, and will most likely be blocked by firewall rules and other anti-fraud mechanisms. Be a good citizen in the OSS community!</p>  <p>If you are using Cirrus CI with your private personal repositories under the $10/month plan you'll have twice the limits:</p> <ul> <li>32.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Windows platform (Containers or VMs)</li> <li>16.0 CPUs for FreeBSD VMs.</li> <li>24.0 CPUs macOS VM (2 VMs).</li> </ul> <p>There are no limits on how many VMs or Containers you can run in parallel if you bring your own infrastructure or use Compute Credits for either private or public repositories.</p>  <p>No per repository limits</p> <p>Cirrus CI doesn't enforce any limits on repository or organization levels. All the limits are on a per-user basis.</p>   <p>Cache and Logs Redundancy</p> <p>By default Cirrus CI persists caches and logs for 90 days. If you bring your own compute services this period can be configured directly in your cloud provider's console.</p>","title":"Are there any limits?"},{"location":"faq/#repository-is-blocked","text":"<p>Free tier of Cirrus CI is intended for public OSS projects to run tests and other validations continuously. If your repository is configured to use Cirrus CI in a questionable way to just exploit Cirrus CI infrastructure, your repository might be blocked.</p> <p>Here are a few examples of such questionable activities we've seen so far:</p> <ul> <li>Use Cirrus CI as a powerhouse for arbitrary CPU-intensive calculations (including crypto mining).</li> <li>Use Cirrus CI to download a pirated movie, re-encode it, upload as a Cirrus artifact and distribute it.</li> <li>Use Cirrus CI distributed infrastructure to emulate user activity on a variety of websites to trick advertisers.</li> </ul>","title":"Repository is blocked"},{"location":"faq/#ip-addresses-of-community-clusters","text":"<p>Instances running on Community Clusters are using dynamic IPs by default. It's possible to request a static <code>35.222.255.190</code> IP for all the community instance types except macOS VMs via <code>use_static_ip</code> field. Here is an example of a Linux Docker container with a static IP:</p> <pre><code>task:\n  name: Test IP\n  container:\n    image: cirrusci/wget:latest\n    use_static_ip: true\n  script: wget -qO- ifconfig.co\n</code></pre>","title":"IP Addresses of Community Clusters"},{"location":"faq/#ci-agent-stopped-responding","text":"<p>It means that Cirrus CI haven't heard from the agent for quite some time. In 99.999% of the cases  it happens because of two reasons:</p> <ol> <li> <p>Your task was executing on Community Cluster. Community Cluster     is backed by Google Cloud's Preemptible VMs for cost efficiency reasons and    Google Cloud preempted back a VM your task was executing on. Cirrus CI is trying to minimize possibility of such cases     by constantly rotating VMs before Google Cloud preempts them, but there is still chance of such inconvenience.</p> </li> <li> <p>Your CI task used too much memory which led to a crash of a VM or a container.</p> </li> </ol>","title":"CI agent stopped responding!"},{"location":"faq/#agent-process-on-a-persistent-worker-exited-unexpectedly","text":"<p>This means that either an agent process or a VM with an agent process exited before reporting the last instruction of a task.</p> <p>If it's happening for a <code>macos_instance</code> then please contact support.</p>","title":"Agent process on a persistent worker exited unexpectedly!"},{"location":"faq/#instance-failed-to-start","text":"<p>It means that Cirrus CI has made a successful API call to a computing service  to allocate resources. But a requested resource wasn't created. </p> <p>If it happened for an OSS project, please contact support immediately. Otherwise check your cloud console first  and then contact support if it's still not clear what happened. </p>","title":"Instance failed to start!"},{"location":"faq/#instance-got-rescheduled","text":"<p>Cirrus CI is trying to be as efficient as possible and heavily uses preemptible VMs to run majority of workloads. It allows to drastically lower Cirrus CI's infrastructure bill and allows to provide the best pricing model with per-second billing and very generous limits for OSS projects, but it comes with a rare edge case... </p> <p>Preemptible VMs can be preempted which will require rescheduling and automatically restart tasks that were executing on these VMs.  This is a rare event since autoscaler is constantly rotating instances but preemption still happens occasionally.  All automatic re-runs and stateful tasks using compute credits are always executed on regular VMs.</p>","title":"Instance got rescheduled!"},{"location":"faq/#instance-timed-out","text":"<p>By default, Cirrus CI has an execution limit of 60 minutes for each task. However, this default timeout duration can be changed by using <code>timeout_in</code> field in <code>.cirrus.yml</code> configuration file:</p> <pre><code>task: \n  timeout_in: 90m\n  ...\n</code></pre>  <p>Maximum timeout</p> <p>There is a hard limit of 2 hours for community tasks. Use compute credits or compute service integration to avoid the limit.</p>","title":"Instance timed out!"},{"location":"faq/#container-errored","text":"<p>It means that Cirrus CI has made a successful API call to a computing service to start a container but unfortunately container runtime or the corresponding computing service had an internal error.</p>","title":"Container errored"},{"location":"faq/#only-github-support","text":"<p>At the moment Cirrus CI only supports GitHub via a GitHub Application. We are planning to support BitBucket next. </p>","title":"Only GitHub Support?"},{"location":"faq/#any-discounts","text":"<p>Cirrus CI itself doesn't provide any discounts except Community Cluster  which is free for open source projects. But since Cirrus CI delegates execution of builds to different computing services, it means that discounts from your cloud provider will be applied to Cirrus CI builds.</p>","title":"Any discounts?"},{"location":"features/","text":"","title":"Features"},{"location":"features/#free-for-open-source","text":"<p>To support the Open Source community, Cirrus CI provides Linux, Windows, macOS and FreeBSD services free of charge with some limits but without a cap on how many minutes a month OSS projects can consume.</p> <p>Here is a list of all instance types available for free for Open Source Projects:</p>    Instance Type Managed by Description     <code>container</code> us Linux Docker Container   <code>arm_container</code> us Linux Arm Docker Container   <code>windows_container</code> us Windows Docker Container   <code>docker_builder</code> us Full-fledged VM pre-configured for running Docker   <code>macos_instance</code> us macOS Virtual Machines   <code>freebsd_instance</code> us FreeBSD Virtual Machines   <code>compute_engine_instance</code> us Full-fledged custom VM   <code>persistent_worker</code> you Use any host on any platform and architecture","title":"Free for Open Source"},{"location":"features/#per-second-billing","text":"<p>Use compute credits to run as many parallel tasks as you want and pay only for CPU time used by these tasks. Another approach is to bring your own infrastructure and pay directly to your cloud provider within your current billing.</p>","title":"Per-second billing"},{"location":"features/#no-concurrency-limit-no-queues","text":"<p>Cirrus CI leverages elasticity of the modern clouds to always have available resources to process your builds. Engineers should never wait for builds to start.</p>","title":"No concurrency limit. No queues"},{"location":"features/#bring-your-own-infrastructure","text":"<p>Cirrus CI supports bringing your own infrastructure (BYO) for full control over security and for easy integration with your current workflow.</p> <p>          </p>","title":"Bring Your Own Infrastructure"},{"location":"features/#flexible-runtime-environment","text":"<p>Cirrus CI allows you to use any Unix or Windows VMs, any Docker containers, any amount of CPUs, optional SSDs and GPUs.</p>","title":"Flexible runtime environment"},{"location":"features/#basic-but-very-powerful-configuration-format","text":"<p>Learn more about how to configure tasks here. Configure things like:</p> <ul> <li>Matrix Builds</li> <li>Dependencies between tasks</li> <li>Conditional Task Execution</li> <li>Local HTTP Cache</li> <li>Dockerfile as a CI environment</li> <li>Monorepo Support</li> </ul> <p>Check the Quick Start guide for more features.</p>","title":"Basic but very powerful configuration format"},{"location":"features/#comparison-with-popular-ciaas","text":"<p>Here is a high level comparison with popular continuous-integration-as-a-service solutions:</p>    Name Linux Windows macOS FreeBSD Customizable CPU/Memory For Open Source For Personal Private Repositories For Organizational Private Repositories     Cirrus CI    (any value) 34 concurrent CPUs with no monthly limit on minutes $10/month with the same OSS limits \ud83d\udc48 Per-second usage with no parallel limitConnect your cloud for $10/month/seat   GitHub Actions    20 concurrent jobs with no monthly limit on minutes 2,000 minutes/month for free Per-minute usage with no parallel limitHost and manage additional runners at no additional cost   Travis CI    1000 minutes per account per month $69/month for 1 concurrent job $49/month per additional concurrency job   CircleCI    (4 types) 40,000 minutes per organization per month 1,000 minutes/month for free$69/month for 2 concurrent job $15/month/user + per-minute usage with up to 80 parallel jobs   AppVeyor    1 concurrent job with no monthly limit on minutes $59/month for 1 concurrent job $50/month per additional concurrency job    <p>Feel free to contact support if you have questions for your particular case.</p>","title":"Comparison with popular CIaaS"},{"location":"pricing/","text":"<p>Cirrus CI is free for Open Source projects with some limitations. For private projects, Cirrus CI has couple of options depending on your needs:</p> <ol> <li>For private personal repositories there is a very affordable $10 a month plan with     access to community clusters for Linux, Windows and macOS workloads.</li> <li>Buy compute credits to access managed and pre-configured community clusters for Linux, FreeBSD, Windows, and macOS workloads.</li> <li>Configure access to your own infrastructure and pay $10/seat/month.</li> </ol> <p>Here is a comparison table of available Cirrus CI plans:</p>    User Free Public Repositories Private Personal Repository Private Organization Repositories     Person <ul><li>Free access to community clusters for public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> <ul><li>Access to community clusters for public and private repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul> Not Applicable   Organization <ul><li>Free access to community clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> Not Applicable <ul><li>Free access to community clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul>","title":"Pricing"},{"location":"pricing/#compute-credits","text":"<p>Sometimes configuring your own compute services isn't worth it. It takes time and effort to maintain them. For such cases there is a way to use the same community clusters that the Open Source community is enjoying. Use compute credits with your private or public repositories of any scale.</p> <p>1 compute credit can be bought for 1 US dollar. Here is how much 1000 minutes of CPU time will cost for different platforms:</p> <ul> <li>1000 minutes of 1 virtual CPU for Linux for 5 compute credits</li> <li>1000 minutes of 1 virtual CPU for FreeBSD for 5 compute credits</li> <li>1000 minutes of 1 virtual CPU for Windows for 10 compute credits</li> <li>1000 minutes of 1 virtual CPU for macOS for 40 compute credits</li> </ul> <p>All tasks using compute credits are charged on per-second basis. 2 CPU Linux task takes 2 minutes? Pay 2 cents.</p> <p>Note: orchestration costs are included in compute credits and there is no need to purchase additional seats on your plan.</p>  <p>Priority Scheduling</p> <p>Tasks that are using compute credits will be prioritized and will be scheduled as fast as possible.</p>   <p>Works for OSS projects</p> <p>Compute credits can be used for commercial OSS projects to avoid concurrency limits. Note that only collaborators for the project will be able to use organization's compute credits.</p>  <p>Benefits of this approach:</p> <ul> <li>Use the same pre-configured infrastructure as the Open Source community is enjoying.</li> <li>No need to configure anything. Let Cirrus CI's team manage and upgrade infrastructure for you.</li> <li>Per-second billing with no additional minimum or monthly fees.</li> <li>Cost efficient for small to medium teams. </li> </ul> <p>Cons of this approach:</p> <ul> <li>No support for exotic use cases like GPUs, SSDs and 100+ cores machines.</li> <li>Not that cost efficient for big teams.</li> </ul>","title":"Compute Credits"},{"location":"pricing/#buying-compute-credits","text":"<p>To see your current balance, recent transactions and to buy more compute credits, go to your organization's settings page:</p> <pre><code>https://cirrus-ci.com/settings/github/MY-ORGANIZATION\n</code></pre>","title":"Buying Compute Credits"},{"location":"pricing/#configuring-compute-credits","text":"<p>Compute credits can be used with any of the following instance types: <code>container</code>, <code>windows_container</code> and <code>macos_instance</code>. No additional configuration needed.</p> amd64arm64   <pre><code>task:\n  container:\n    image: node:latest\n  ...\n</code></pre>   <pre><code>task:\n  arm_container:\n    image: node:latest\n  ...\n</code></pre>     <p>Using compute credits for public or personal private repositories</p> <p>If you willing to boost Cirrus CI for public or your personal private repositories you need to explicitly mark a task to use compute credits with <code>use_compute_credits</code> field.</p> <p>Here is an example of how to enable compute credits for internal and external collaborators of a public repository:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_USER_COLLABORATOR == 'true'\n</code></pre> <p>Here is another example of how to enable compute credits for master branch of a personal private project to make sure all of the master builds are executed as fast as possible by skipping community clusters usage limits:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_BRANCH == 'master'\n</code></pre>","title":"Configuring Compute Credits"},{"location":"pricing/#compute-services","text":"<p>Configure and connect one or more compute services and/or persistent workers to Cirrus CI for orchestrating CI workloads on them. It's free for your public repositories and costs $10/seat/month to use with private repositories.</p> <p>Benefits of this approach:</p> <ul> <li>Full control of underlying infrastructure. Use any type of VMs and containers with any amount of CPUs and memory.</li> <li>More secure. Setup any firewall and access rules.</li> <li>Pay for CI within your existing cloud and GitHub bills. </li> </ul> <p>Cons of this approach:</p> <ul> <li>Need to configure and connect one or several compute services.</li> <li>Might not be worth the effort for a small team.</li> <li>Need to pay $10/seat/month plan.</li> </ul>  <p>What is a seat?</p> <p>A seat is a GitHub user that initiates CI builds by pushing commits and/or creating pull requests in a private repository.  It can be a real person or a bot. If you are using Cron Builds or creating builds through Cirrus's API it will be counted as an additional seat (like a bot).</p> <p>For example, if there are 10 people in your GitHub Organization and only 5 of them are working on private repositories  where Cirrus CI is configured, the remaining 5 people are not counted as seats, given that they aren't pushing to the private repository.  Let's say Dependabot is also configured for these private repositories. </p> <p>In that case there are <code>5 + 1 = 6</code> seats you need to purchase Cirrus CI plan for.</p>","title":"Compute Services"},{"location":"security/","text":"","title":"Security Policy"},{"location":"security/#reporting-a-vulnerability","text":"<p>If you find a security vulnerability in the Cirrus CI platform (the backend, web interface, etc.), please follow the steps below.</p> <ol> <li>Do NOT comment about the vulnerability publicly.</li> <li> <p>Please email <code>hello@cirruslabs.org</code> with the following format:</p> <pre><code>Subject: Platform Security Risk\n\nHOW TO EXPLOIT\n\nGive exact details so our team can replicate it.\n\nOTHER INFORMATION\n\nIf anything else needs to be said, put it here.\n</code></pre> </li> <li> <p>Please be patient. You will get an email back soon.</p> </li> </ol> <p>Thank you! </p>","title":"Reporting a Vulnerability"},{"location":"support/","text":"<p>The best way to ask general questions about a particular use cases is to email us at support+ci@cirruslabs.org.</p> <p>If you have a feature request or noticed lack of some documentation please feel free to create a GitHub issue. Our support team will answer it by replying or updating documentation.</p>","title":"Support"},{"location":"support/#migration-assistance","text":"<p>Cirrus Labs can help your team with migration to Cirrus CI. Our team will analyze your current needs and workflow in order to not only perform the migration, but also perform optimizations along the way to make sure your team's workflow is as  optimal as it can possibly be.</p> <p>Our team has experience of optimizing developer experiences at companies like Airbnb and Twitter. Contact us at support+migration@cirruslabs.org.</p>","title":"Migration Assistance"},{"location":"guide/FreeBSD/","text":"","title":"FreeBSD VMs"},{"location":"guide/FreeBSD/#freebsd-virtual-machines","text":"<p>It is possible to run FreeBSD Virtual Machines the same way one can run Linux containers on the FreeBSD Community Cluster.  To accomplish this, use <code>freebsd_instance</code> in your <code>.cirrus.yml</code>:</p> <pre><code>freebsd_instance:\n  image_family: freebsd-13-0\n\ntask:\n  install_script: pkg install -y ...\n  script: ...\n</code></pre>  <p>Under the Hood</p> <p>Under the hood, a basic integration with Google Compute Engine  is used and <code>freebsd_instance</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> <pre><code>compute_engine_instance:\n  image_project: freebsd-org-cloud-dev\n  image: family/freebsd-13-0\n  platform: freebsd\n</code></pre>","title":"FreeBSD Virtual Machines"},{"location":"guide/FreeBSD/#list-of-available-image-families","text":"<p>Any of the official FreeBSD VMs on Google Cloud Platform are supported. Here are a few of them which are self explanatory:</p> <ul> <li><code>freebsd-14-0-snap</code> (14.0-SNAP)</li> <li><code>freebsd-13-0</code>      (13.0-RELEASE)</li> <li><code>freebsd-12-3</code>      (12.3-RELEASE)</li> <li><code>freebsd-12-2</code>      (12.2-RELEASE)</li> <li><code>freebsd-12-0</code>      (12.0-RELEASE)</li> <li><code>freebsd-11-4</code>      (11.4-RELEASE)</li> <li><code>freebsd-11-3-snap</code> (11.3-STABLE)</li> <li><code>freebsd-11-3</code>      (11.3-RELEASE, doesn't boot properly at the moment)</li> </ul> <p>It's also possible to specify a concrete version of an image by name via <code>image_name</code> field. To get a full list of available images please run the following gcloud command:</p> <pre><code>gcloud compute images list --project freebsd-org-cloud-dev --no-standard-images\n</code></pre>","title":"List of available image families"},{"location":"guide/build-life/","text":"<p>Any build starts with a change pushed to GitHub. Since Cirrus CI is a GitHub Application, a webhook event  will be triggered by GitHub. From the webhook event, Cirrus CI will parse a Git branch and the SHA  for the change. Based on said information, a new build will be created.</p> <p>After build creation Cirrus CI will use GitHub's APIs to download a content of <code>.cirrus.yml</code> file for the SHA. Cirrus CI will evaluate it and create corresponding tasks.</p> <p>These tasks (defined in the <code>.cirrus.yml</code> file) will be dispatched within Cirrus CI to different services responsible for scheduling on a supported computing service. Cirrus CI's scheduling service will use appropriate APIs to create and manage a VM instance or a Docker container on the particular computing service.  The scheduling service will also configure start-up script that downloads the Cirrus CI agent, configures it to send logs back and starts it. Cirrus CI agent is a self-contained executable written in Go which means it can be executed anywhere.</p> <p>Cirrus CI's agent will request commands to execute for a particular task and will stream back logs, caches, artifacts and exit codes of the commands upon execution. Once the task finishes, the scheduling service will clean up the used VM or container.</p> <p></p> <p>This is a diagram of how Cirrus CI schedules a task on Google Cloud Platform (the community cluster's engine). The blue arrows represent API calls and the green arrows represent unidirectional communication between an agent inside a VM or a container and Cirrus CI. Other chores such as health checking of the agent and GitHub status reporting happen in real time as a task is running.</p> <p>Straight forward and nothing magical. </p>  <p>For any questions, feel free to contact us.</p>","title":"Life of a Build"},{"location":"guide/custom-vms/","text":"","title":"Custom VMs"},{"location":"guide/custom-vms/#custom-compute-engine-vms","text":"<p>Cirrus CI supports many different compute services when you bring your own infrastructure,  but internally at Cirrus Labs we use Google Cloud Platform for running all managed by us instances except <code>macos_instance</code>. Already things like Docker Builder and <code>freebsd_instance</code> are basically a syntactic sugar for launching Compute Engine instances from a particular limited set of images.</p> <p>With <code>compute_engine_instance</code> it is possible to use any publicly available image for running your Cirrus tasks in. Such instances are particularly useful when you can't use Docker containers, for example, when you need to test things against newer versions of Linux kernel then the Docker host has.</p> <p>Here is an example of using a <code>compute_engine_instance</code> to run a VM with KVM available:</p> <pre><code>compute_engine_instance:\n  image_project: cirrus-images # GCP project\n  image: family/docker-kvm # family or a full image name.\n  platform: linux\n  cpu: 4 # optional. Defaults to 2 CPUs.\n  memory: 16G # optional. Defaults to 4G.\n  disk: 100 # optional. By default, uses the smallest disk size required by the image.\n  nested_virtualization: true # optional. Whether to enable Intel VT-x. Defaults to false.\n</code></pre>  Nested Virtualization License <p>Make sure that your source image already has a necessary license. Otherwise, nested virtualization won't work.</p>","title":"Custom Compute Engine VMs"},{"location":"guide/custom-vms/#building-custom-image-for-compute-engine","text":"<p>We recommend to use Packer for building your custom images. As an example, please take a look at our Packer templates used for building Docker Builder VM image.</p> <p>After building your image, please make sure the image publicly available:</p> <pre><code>gcloud compute images add-iam-policy-binding $IMAGE_NAME \\\n    --member='allAuthenticatedUsers' \\\n    --role='roles/compute.imageUser'\n</code></pre>","title":"Building custom image for Compute Engine"},{"location":"guide/docker-builder-vm/","text":"","title":"Docker Builder on VM"},{"location":"guide/docker-builder-vm/#docker-builder-vm","text":"<p>\"Docker Builder\" tasks are a way to build and publish Docker Images to Docker Registries of your choice using a VM as build environment. In essence, a <code>docker_builder</code> is basically a <code>task</code> that is executed in a VM with pre-installed Docker.  A <code>docker_builder</code> can be defined the same way as a <code>task</code>:</p> <pre><code>docker_builder:\n  build_script: docker build --tag myrepo/foo:latest .\n</code></pre> <p>Leveraging features such as Task Dependencies, Conditional Execution and Encrypted Variables with a Docker Builder can help building relatively complex pipelines. It can also be used to execute builds which need special privileges.</p> <p>In the example below, a <code>docker_builder</code> will be only executed on a tag creation, once both <code>test</code> and <code>lint</code>  tasks have finished successfully:</p> <pre><code>test_task: ...\nlint_task: ...\n\ndocker_builder:\n  only_if: $CIRRUS_TAG != ''\n  depends_on: \n    - test\n    - lint\n  env:\n    DOCKER_USERNAME: ENCRYPTED[...]\n    DOCKER_PASSWORD: ENCRYPTED[...]\n  build_script: docker build --tag myrepo/foo:$CIRRUS_TAG .\n  login_script: docker login --username $DOCKER_USERNAME --password $DOCKER_PASSWORD\n  push_script: docker push myrepo/foo:$CIRRUS_TAG\n</code></pre>  <p>Example</p> <p>For more examples please check how we use Docker Builder to build and publish Cirrus CI's Docker Images for Android.</p>","title":"Docker Builder VM"},{"location":"guide/docker-builder-vm/#multi-arch-builds","text":"<p>Docker Builder VM has QEMU pre-installed and is able to execute multi-arch builds via <code>buildx</code>. Add the following <code>setup_script</code> to enable <code>buildx</code> and then use <code>docker buildx build</code> instead of the regular <code>docker build</code>:</p> <pre><code>docker_builder:\n  setup_script:\n    - docker buildx create --name multibuilder\n    - docker buildx use multibuilder\n    - docker buildx inspect --bootstrap\n  build_script: docker buildx build --platform linux/amd64,linux/arm64 --tag myrepo/foo:$CIRRUS_TAG .\n</code></pre>","title":"Multi-arch builds"},{"location":"guide/docker-builder-vm/#pre-installed-packages","text":"<p>For your convenience, a Docker Builder VM has some common packages pre-installed:</p> <ul> <li>AWS CLI</li> <li>Docker Compose</li> <li>Heroku CLI</li> <li>OpenJDK 11</li> <li>Python</li> <li>Ruby with Bundler</li> </ul>","title":"Pre-installed Packages"},{"location":"guide/docker-builder-vm/#under-the-hood","text":"<p>Under the hood a simple integration with Google Compute Engine is used and basically <code>docker_builder</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> <pre><code>task:\n  compute_engine_instance:\n    image_project: cirrus-images\n    image: family/docker-builder\n    platform: linux\n    cpu: 4\n    memory: 16G\n</code></pre> <p>You can check Packer templates of the VM image in <code>cirruslabs/vm-images</code> repository.</p>","title":"Under the hood"},{"location":"guide/docker-builder-vm/#layer-caching","text":"<p>Docker has the <code>--cache-from</code> flag which allows using a previously built image as a cache source. This way only changed layers will be rebuilt which can drastically improve performance of the <code>build_script</code>. Here is a snippet that uses  the <code>--cache-from</code> flag:</p> <pre><code># pull an image if available\ndocker pull myrepo/foo:latest || true\ndocker build --cache-from myrepo/foo:latest \\\n  --tag myrepo/foo:$CIRRUS_TAG \\\n  --tag myrepo/foo:latest .\n</code></pre>","title":"Layer Caching"},{"location":"guide/docker-builder-vm/#dockerfile-as-a-ci-environment","text":"<p>With Docker Builder there is no need to build and push custom containers so they can be used as an environment to run CI tasks in.  Cirrus CI can do it for you! Just declare a path to a <code>Dockerfile</code> with the <code>dockerfile</code> field for your <code>container</code> (<code>arm_container</code>s are not supported yet) declaration in your <code>.cirrus.yml</code> like this:</p> <pre><code>efficient_task:\n  container:\n    dockerfile: ci/Dockerfile\n    docker_arguments:\n      foo: bar\n  test_script: ...\n\ninefficient_task:\n  container:\n    image: node:latest\n  setup_script:\n    - apt-get update\n    - apt-get install build-essential\n  test_script: ...\n</code></pre> <p>Cirrus CI will build a container and cache the resulting image based on <code>Dockerfile</code>\u2019s content. On the next build,  Cirrus CI will check if a container was already built, and if so, Cirrus CI will instantly start a CI task using the cached image.</p> <p>Under the hood, for every <code>Dockerfile</code> that is needed to be built, Cirrus CI will create a Docker Builder task as a dependency.  You will see such <code>build_docker_image_HASH</code> tasks in the UI.</p>  <p>Danger of using <code>COPY</code> and <code>ADD</code> instructions</p> <p>Cirrus doesn't include files added or copied into a container image in the cache key. This means that for a public repository a potential bad actor can create a PR with malicious scripts included into a container, wait for it to be cached and then reset the PR so it looks harmless.</p>   Using with private GKE clusters <p>To use <code>dockerfile</code> with <code>gke_container</code> you first need to create a VM with Docker installed within your GCP project. This image will be used to perform building of Docker images for caching. Once this image is available, for example, by  <code>MY_DOCKER_VM</code> name, you can use it like this:</p> <pre><code>gke_container:\n  dockerfile: .ci/Dockerfile\n  builder_image_name: MY_DOCKER_VM\n  cluster_name: cirrus-ci-cluster\n  zone: us-central1-a\n  namespace: default\n</code></pre> <p>If your builder image is stored in another project you can also specify it by using <code>builder_image_project</code> field. By default, Cirrus CI assumes builder image is stored within the same project as the GKE cluster.</p>","title":"Dockerfile as a CI environment"},{"location":"guide/docker-builder-vm/#windows-support","text":"<p>Docker builders also support building Windows Docker containers - use the <code>platform</code> and <code>os_version</code> fields:</p> <pre><code>docker_builder:\n  platform: windows\n  os_version: 2019\n  ...\n</code></pre>  <p>Supported OS Versions</p> <p>See Windows Containers documentation for a list of supported OS versions.</p>","title":"Windows Support"},{"location":"guide/docker-builds-on-kubernetes/","text":"","title":"Docker Builds on GKE"},{"location":"guide/docker-builds-on-kubernetes/#docker-builds-on-kubernetes","text":"<p>Besides the ability to build docker images using a dedicated <code>docker_builder</code> task which runs on VMs, it is also possible to run docker builds on Kubernetes. To do so we are leveraging the <code>additional_containers</code> and <code>docker-in-docker</code> functionality.</p> <p>Currently Cirrus CI supports running builds on these Kubernetes distributions:</p> <ul> <li>Google Kubernetes Engine (GKE)</li> <li>AWS Elastic Kubernetes Service (EKS)</li> </ul> <p>For Generic Kubernetes Support follow this issue.</p>","title":"Docker Builds on Kubernetes"},{"location":"guide/docker-builds-on-kubernetes/#comparison-of-docker-builds-on-vms-vs-kubernetes","text":"<ul> <li>VMs<ul> <li>complex builds are potentially faster than <code>docker-in-docker</code></li> <li>safer due to better isolation between builds</li> </ul> </li> <li>Kubernetes<ul> <li>much faster start - creating a new container usually takes few seconds vs creating a VM which takes usually about a minute on GCP and even longer on AWS.</li> <li>ability to use an image with your custom tools image (e.g. containing Skaffold) to invoke docker instead of using a fixed VM image.</li> </ul> </li> </ul>","title":"Comparison of docker builds on VMs vs Kubernetes"},{"location":"guide/docker-builds-on-kubernetes/#how-to","text":"<p>This a full example of how to build a docker image on GKE using docker and pushing it to GCR. While not required, the script section in this example also has some best practice cache optimizations and pushes the image to GCR.</p>  <p>AWS EKS support</p> <p>While the steps below are specifically written for and tested with GKE (Google Kubernetes Engine), it should work equally on AWS EKS.</p>  <pre><code>docker_build_task:\n  gke_container: # for AWS, replace this with `aks_container`\n    image: docker:latest # This image can be any custom image. The only hard requirement is that it needs to have `docker-cli` installed.\n    cluster_name: cirrus-ci-cluster # your gke cluster name\n    zone: us-central1-b # zone of the cluster\n    namespace: cirrus-ci # namespace to use\n    cpu: 1\n    memory: 1500Mb\n    additional_containers:\n      - name: dockerdaemon\n        privileged: true # docker-in-docker needs to run in privileged mode\n        cpu: 4\n        memory: 3500Mb\n        image: docker:dind\n        port: 2375\n        env:\n          DOCKER_DRIVER: overlay2 # this speeds up the build\n  env:\n    DOCKER_HOST: tcp://localhost:2375 # this is required so that docker cli commands connect to the \"additional container\" instead of `docker.sock`.\n    GOOGLE_CREDENTIALS: ENCRYPTED[qwerty239abc] # this should contain the json key for a gcp service account with the `roles/storage.admin` role on the `artifacts.&lt;your_gcp_project&gt;.appspot.com` bucket as described here https://cloud.google.com/container-registry/docs/access-control. This is only required if you want to pull / push to gcr. If we use dockerhub you need to use different credentials.\n  login_script:\n    echo $GOOGLE_CREDENTIALS | docker login -u _json_key --password-stdin https://gcr.io\n  build_script:\n    - docker pull gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE || true\n    - docker build\n      --cache-from=gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE\n      -t gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n      .   \n  push_script:\n    - docker push gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n</code></pre>","title":"How to"},{"location":"guide/docker-builds-on-kubernetes/#caveats","text":"<p>Since the <code>additional_container</code> needs to run in privileged mode, the isolation between the Docker build and the host are somewhat limited, you should create a separate cluster for Cirrus CI builds ideally. If this a concern you can also try out Kaniko or Makisu to run builds in unprivileged containers.</p>","title":"Caveats"},{"location":"guide/docker-pipe/","text":"<p>Docker Pipe is a way to execute each instruction in its own Docker container while persisting working directory between each of the containers. For example, you can build your application in  one container, run some lint tools in another containers and finally deploy your app via CLI from another container.</p> <p>No need to create huge containers with every single tool pre-installed!</p> <p>A <code>pipe</code> can be defined the same way as a <code>task</code> with the only difference that instructions should be grouped under the <code>steps</code> field defining a Docker <code>image</code> for each step to be executed in. Here is an example of how we build and validate links for the Cirrus CI documentation that you are reading right now:</p> <pre><code>pipe:\n  name: Build Site and Validate Links\n  steps:\n    - image: squidfunk/mkdocs-material:latest\n      build_script: mkdocs build\n    - image: raviqqe/liche:latest # links validation tool in a separate container\n      validate_script: /liche --document-root=site --recursive site/\n</code></pre> <p>Amount of CPU and memory that a pipe has access to can be configured with <code>resources</code> field:</p> <pre><code>pipe:\n  resources:\n    cpu: 2.5\n    memory: 5G\n  # ...\n</code></pre>","title":"Docker Pipe"},{"location":"guide/linux/","text":"","title":"Linux Containers"},{"location":"guide/linux/#linux-containers","text":"<p>Cirrus CI supports <code>container</code> and <code>arm_container</code> instances in order to run your CI workloads on <code>amd64</code> and <code>arm64</code> platforms respectively. Cirrus CI uses Kubernetes clusters running in different clouds that are the most suitable for running each platform:</p> <ul> <li>For <code>container</code> instances Cirrus CI uses a GKE cluster of compute-optimized instances running in Google Cloud.</li> <li>For <code>arm_container</code> instances Cirrus CI uses a EKS cluster of Graviton2 instances running in AWS.</li> </ul> <p>Community Clusters are configured the same way as anyone can configure a private Kubernetes cluster for their own repository. Cirrus CI supports connecting managed Kubernetes clusters from most of the cloud providers. Please check out all the supported computing services Cirrus CI can integrate with.</p> <p>By default, a container is given 2 CPUs and 4 GB of memory, but it can be configured in <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre>   <pre><code>arm_container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre>    <p>Containers on Community Cluster can use maximum 8.0 CPUs and up to 32 GB of memory. Memory limit is tied to the amount of CPUs requested. For each CPU you can't get more than 4G of memory.</p> <p>Tasks using Compute Credits has higher limits and can use up to 28.0 CPUs and 112G of memory respectively.</p>  Scheduling Times on Community Cluster <p>Since Community Cluster is shared, scheduling times for containers can vary from time to time. Also, the smaller a container  require resources the faster it will be scheduled.</p> <p>If you have a popular project and experiencing long scheduling times, don't hesitate to reach out to support and we can whitelist your repository for use of extra resources.</p>   Using in-memory disks <p>Some I/O intensive tasks may benefit from using a <code>tmpfs</code> disk mounted as a working directory. Set <code>use_in_memory_disk</code> flag to enable in-memory disk for a container:</p> amd64arm64   <pre><code>task:\n  name: Much I/O\n  container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre>   <pre><code>task:\n  name: Much I/O\n  arm_container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre>    <p>Note: any files you write including cloned repository will count against your task's memory limit.</p>   Privileged Access <p>If you need to run privileged docker containers, take a look at the docker builder.</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","title":"Linux Containers"},{"location":"guide/linux/#kvm-enabled-privileged-containers","text":"<p>It is possible to run containers with KVM enabled. Some types of CI tasks can tremendously benefit from native virtualization. For example, Android related tasks can benefit from running hardware accelerated emulators instead of software emulated ARM emulators.</p> <p>In order to enable KVM module for your <code>container</code>s, add <code>kvm: true</code> to your <code>container</code> declaration. Here is an example of a task that runs hardware accelerated Android emulators:</p> <pre><code>task:\n  name: Integration Tests (x86)\n  container:\n    image: cirrusci/android-sdk:29\n    kvm: true\n  accel_check_script: emulator -accel-check\n</code></pre>  <p>Limitations of KVM-enabled Containers</p> <p>Because of the additional virtualization layer, it takes about a minute to acquire the necessary resources to start such tasks. KVM-enabled containers are backed by dedicated VMs which restrict the amount of CPU resources that can be used. The value of <code>cpu</code> must be <code>1</code> or an even integer. Values like <code>0.5</code> or <code>3</code> are not supported for KVM-enabled containers </p>","title":"KVM-enabled Privileged Containers"},{"location":"guide/linux/#working-with-private-registries","text":"<p>It is possible to use private Docker registries with Cirrus CI to pull containers. To provide an access to a private registry  of your choice you'll need to obtain a JSON Docker config file for your registry and create an encrypted variable for Cirrus CI to use.</p> <p>Let's check an example of setting up Oracle Container Registry in order to use Oracle Database in tests.</p> <p>First, you'll need to login with the registry by running the following command:</p> <pre><code>docker login container-registry.oracle.com\n</code></pre> <p>After a successful login, Docker config file located in <code>~/.docker/config.json</code> will look something like this:</p> <pre><code>{\n  \"auths\": {\n    \"container-registry.oracle.com\": {\n      \"auth\": \"....\"\n    }\n  }\n}\n</code></pre> <p>If you don't see <code>auth</code> for your registry, it means your Docker installation is using a credentials store. In this case you can manually auth using a Base64 encoded string of your username and your PAT (Personal Access Token). Here's how to generate that:</p> <pre><code>echo $USERNAME:$PAT | base64\n</code></pre> <p>Create an encrypted variable from the Docker config and put in <code>.cirrus.yml</code>:</p> <pre><code>registry_config: ENCRYPTED[...]\n</code></pre> <p>Now Cirrus CI will be able to pull images from Oracle Container Registry:</p> <pre><code>registry_config: ENCRYPTED[...]\n\ntest_task:\n  container:\n    image: bigtruedata/sbt:latest\n    additional_containers:\n      - name: oracle\n        image: container-registry.oracle.com/database/standard:latest\n        port: 1521\n        cpu: 1\n        memory: 8G\n   build_script: ./build/build.sh\n</code></pre>","title":"Working with Private Registries"},{"location":"guide/macOS/","text":"","title":"macOS VMs"},{"location":"guide/macOS/#macos-virtual-machines","text":"<p>It is possible to run macOS Virtual Machines (like how one can run Linux containers) on the macOS Community Cluster.  Use <code>macos_instance</code> in your <code>.cirrus.yml</code> files:</p> <pre><code>macos_instance:\n  image: monterey-base\n\ntask:\n  script: echo \"Hello World from macOS!\"\n</code></pre>","title":"macOS Virtual Machines"},{"location":"guide/macOS/#list-of-available-images","text":"","title":"List of available images"},{"location":"guide/macOS/#macos-monterey","text":"<ul> <li><code>monterey-base</code> - vanilla macOS with Brew and Command Line Tools pre-installed.</li> <li><code>monterey-xcode-NN</code> - based off <code>monterey-base</code> with Xcode NN and couple other packages pre-installed:    <code>cocoapods</code>, <code>fastlane</code>, <code>rake</code> and <code>xctool</code>. Flutter and Android SDK/NDK are also pre-installed.</li> </ul> <p>List of available Xcode versions:</p> <ul> <li><code>monterey-xcode-13.1</code></li> <li><code>monterey-xcode-13.2</code></li> </ul> <p>Note that there is a <code>monterey-xcode</code> alias available to always reference to the latest stable <code>monterey-xcode-NN</code> image.</p>","title":"macOS Monterey"},{"location":"guide/macOS/#macos-big-sur","text":"<ul> <li><code>big-sur-base</code> - vanilla macOS with Brew and Command Line Tools pre-installed.</li> <li><code>big-sur-xcode-NN</code> - based off <code>big-sur-base</code> with Xcode NN and couple other packages pre-installed:    <code>cocoapods</code>, <code>fastlane</code>, <code>rake</code> and <code>xctool</code>. Flutter and Android SDK/NDK are also pre-installed.</li> </ul> <p>List of available Xcode versions:</p> <ul> <li><code>big-sur-xcode-12.3</code></li> <li><code>big-sur-xcode-12.4</code></li> <li><code>big-sur-xcode-12.5</code></li> <li><code>big-sur-xcode-13</code></li> </ul> <p>Note that there is a <code>big-sur-xcode</code> alias available to always reference to the latest stable <code>big-sur-xcode-NN</code> image.</p>","title":"macOS Big Sur"},{"location":"guide/macOS/#macos-catalina-deprecated","text":"<ul> <li><code>catalina-base</code> - vanilla macOS with Brew and Command Line Tools pre-installed.</li> <li><code>catalina-xcode-NN</code> - based off <code>catalina-base</code> with Xcode NN and couple other packages pre-installed:    <code>cocoapods</code>, <code>fastlane</code>, <code>rake</code> and <code>xctool</code>. Starting from Xcode 12.1 Flutter and Android SDK/NDK are also pre-installed.</li> <li><code>catalina-xcode-NN-flutter</code> (deprecated since Xcode 12.1) - based of <code>catalina-xcode-NN</code> with pre-installed Flutter and Android SDK/NDK.</li> </ul> <p>List of available Xcode versions:</p> <ul> <li><code>catalina-xcode-11.3.1</code></li> <li><code>catalina-xcode-11.4.1</code></li> <li><code>catalina-xcode-11.5</code></li> <li><code>catalina-xcode-11.6</code></li> <li><code>catalina-xcode-12.0</code></li> <li><code>catalina-xcode-12.1</code></li> <li><code>catalina-xcode-12.2</code></li> </ul> <p>Note that there are a couple of aliases available for images:</p> <ul> <li><code>catalina-xcode</code> - point to the latest stable <code>catalina-xcode-NN</code> image.</li> <li><code>catalina-flutter</code> - point to the latest image with.</li> </ul>","title":"macOS Catalina (deprecated)"},{"location":"guide/macOS/#how-images-are-built","text":"<p>Please refer to the <code>osx-images</code> repository on how the images were built and don't hesitate to create issues if current images are missing something.</p>  <p>Underlying Technology</p> <p>Under the hood Cirrus CI is using Cirrus CI's own Persistent Workers. See more details in out blog post.</p>","title":"How images are built"},{"location":"guide/notifications/","text":"<p>Cirrus CI itself doesn't have built-in mechanism to send notifications but, since Cirrus CI is following best practices of integrating with GitHub, it's possible to configure a GitHub action that will send any kind of notifications.</p> <p>Here is a full list of curated Cirrus Actions for GitHub including ones to send notifications: cirrus-actions.</p>","title":"Notifications"},{"location":"guide/notifications/#email-action","text":"<p>It's possible to facilitate GitHub Action's own email notification mechanism to send emails about Cirrus CI failures.  To enable it, add the following <code>.github/workflows/email.yml</code> workflow file:</p> <pre><code>on:\n  check_suite:\n    type: ['completed']\n\nname: Email about Cirrus CI failures\njobs:\n  continue:\n    name: After Cirrus CI Failure\n    if: github.event.check_suite.app.name == 'Cirrus CI' &amp;&amp; github.event.check_suite.conclusion != 'success'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: octokit/request-action@v2.x\n        id: get_failed_check_run\n        with:\n          route: GET /repos/${{ github.repository }}/check-suites/${{ github.event.check_suite.id }}/check-runs?status=completed\n          mediaType: '{\"previews\": [\"antiope\"]}'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - run: |\n          echo \"Cirrus CI ${{ github.event.check_suite.conclusion }} on ${{ github.event.check_suite.head_branch }} branch!\"\n          echo \"SHA ${{ github.event.check_suite.head_sha }}\"\n          echo $MESSAGE\n          echo \"##[error]See $CHECK_RUN_URL for details\" &amp;&amp; false\n        env:\n          CHECK_RUN_URL: ${{ fromJson(steps.get_failed_check_run.outputs.data).check_runs[0].html_url }}\n</code></pre>","title":"Email Action"},{"location":"guide/persistent-workers/","text":"","title":"Persistent Workers"},{"location":"guide/persistent-workers/#persistent-workers","text":"","title":"Persistent Workers"},{"location":"guide/persistent-workers/#introduction","text":"<p>Cirrus CI pioneered an idea of directly using compute services instead of requiring users to manage their own infrastructure, configuring servers for running CI jobs, performing upgrades, etc. Instead, Cirrus CI just uses APIs of cloud providers to create virtual machines or containers on demand. This fundamental design difference has multiple benefits comparing to more traditional CIs:</p> <ol> <li>Ephemeral environment. Each Cirrus CI task starts in a fresh VM or a container without any state left by previous tasks.</li> <li>Infrastructure as code. All VM versions and container tags are specified in <code>.cirrus.yml</code> configuration file in your Git repository.    For any revision in the past Cirrus tasks can be identically reproduced at any point in time in the future using the exact versions of VMs or container tags specified in <code>.cirrus.yml</code> at the particular revision. Just imagine how difficult it is to do a security release for a 6 months old version if your CI environment independently changes.</li> <li>Predictability and cost efficiency. Cirrus CI uses elasticity of modern clouds and creates VMs and containers on demand    only when they are needed for executing Cirrus tasks and deletes them right after. Immediately scale from 0 to hundreds or    thousands of parallel Cirrus tasks without a need to over provision infrastructure or constantly monitor if your team has reached maximum parallelism of your current CI plan.</li> </ol>","title":"Introduction"},{"location":"guide/persistent-workers/#what-is-a-persistent-worker","text":"<p>For some use cases the traditional CI setup is still useful. However, not everything is available in the cloud. For example, Apple releases new ARM-based products and there is no virtualization yet available for the new hardware.  Another use case is to test the hardware itself, since not everyone is working on websites and mobile apps after all! For such use cases it makes sense to go with a traditional CI setup: install some binary on the hardware which will constantly pull for new tasks  and will execute them one after another.</p> <p>This is precisely what Persistent Workers for Cirrus CI are: a simple way to run Cirrus tasks beyond cloud!</p>","title":"What is a Persistent Worker"},{"location":"guide/persistent-workers/#configuration","text":"<p>First, create a persistent workers pool for your personal account or a GitHub organization (<code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>):</p> <p></p> <p>Once a persistent worker is created, copy registration token of the pool and follow Cirrus CLI guide to configure a host that will be a persistent worker.</p> <p>Once configured, target task execution on a worker by using <code>persistent_worker</code> instance and matching by workers' labels:</p> <pre><code>task:\n  persistent_worker:\n    labels:\n      os: darwin\n      arch: arm64\n  script: echo \"running on-premise\"\n</code></pre> <p>Or remove <code>labels</code> filed if you want to target any worker:</p> <pre><code>task:\n  persistent_worker: {}\n  script: echo \"running on-premise\"\n</code></pre>","title":"Configuration"},{"location":"guide/persistent-workers/#isolation","text":"<p>By default, a persistent worker spawns all the tasks on the same host machine it's being run.</p> <p>However, using the <code>isolation</code> field, a persistent worker can utilize a VM or a container engine to increase the separation between tasks and to unlock the ability to use different operating systems.</p>","title":"Isolation"},{"location":"guide/persistent-workers/#parallels","text":"<p>To use this isolation type, install the Parallels Desktop on the persistent worker's host machine and create a base VM that will be later cloned for each task.</p> <p>This base VM needs to:</p> <ul> <li>be either in a stopped or suspended state</li> <li>provide SSH access on port 22</li> </ul> <p>Here's an example of a configuration that will run the task inside of a fresh macOS virtual machine created from the <code>big-sur-base</code> base VM:</p> <pre><code>persistent_worker:\n  isolation:\n    parallels:\n      image: big-sur-base\n      user: admin\n      password: secret\n      platform: darwin\n\ntask:\n  script: system_profiler\n</code></pre> <p>Once the VM spins up, persistent worker will connect to the VM's IP-address over SSH using <code>user</code> and <code>password</code> credentials and run the latest agent version targeted for the <code>platform</code>.</p>","title":"Parallels"},{"location":"guide/persistent-workers/#container","text":"<p>To use this isolation type, install and configure a container engine like Docker or Podman (essentially the ones supported by the Cirrus CLI).</p> <p>Here's an example that runs a task in a separate container with a couple directories from the host machine being accessible:</p> <pre><code>persistent_worker:\n  isolation:\n    container:\n      image: debian:latest\n      cpu: 24\n      memory: 128G\n      volumes:\n        - /path/on/host:/path/in/container\n        - /tmp/persistent-cache:/tmp/cache:ro\n\ntask:\n  script: uname -a\n</code></pre>","title":"Container"},{"location":"guide/programming-tasks/","text":"","title":"Programming Tasks in Starlark"},{"location":"guide/programming-tasks/#introduction-into-starlark","text":"<p>Most commonly, Cirrus tasks are declared in <code>.cirrus.yml</code> file in YAML format as documented in Writing Tasks guide.</p> <p>YAML, as a language, is great for declaring simple to moderate configurations, but sometimes just using a declarative language is not enough. One might need some conditional execution or have an easy way to generate multiple similar tasks. Most of the CIs solve this problem by introducing special DSL into the existing YAML. In case of Cirrus CI, we have <code>only_if</code> keyword for conditional execution and <code>matrix</code> modification for generating similar tasks. These options are mostly hacks to workaround declarative nature of YAML language where in reality an imperative language looks like a better fit. This is why Cirrus CI allows in additional to YAML configure tasks via Starlark.</p> <p>Starlark language is a procedural programming language originated from Bazel build tool, but ideal for embedding within any other system that want to safely allow user-defined logic. There are a few key differences which made us choose Starlark instead of common alternatives like JavaScript/TypeScript or WebAssembly:</p> <ol> <li>Starlark doesn't require compilation. No need to introduce full-blown compile and deploy process for a few dozen lines of logic.</li> <li>Starlark script can be executed instantly on any platform. There is Starlark interpreter written in Go which integrates nicely with Cirrus CLI and Cirrus CI infrastructure.</li> <li>Starlark has built-in functionality for loading external modules which is ideal for config sharing. See module loading for details.</li> </ol>","title":"Introduction into Starlark"},{"location":"guide/programming-tasks/#writing-starlark-scripts","text":"<p>Let's start with a trivial <code>.cirrus.star</code> example like this:</p> <pre><code>def main():\n    return [\n        {\n            \"container\": {\n                \"image\": \"debian:latest\",\n            },\n            \"script\": \"make\",\n        },\n    ]\n</code></pre> <p>With the module loading, you can re-use other people's code to avoid wasting time on things written from scratch. For example, with the official task helpers the example above can be refactored in:</p> <pre><code>load(\"github.com/cirrus-modules/helpers\", \"task\", \"container\", \"script\")\n\ndef main(ctx):\n  return [\n    task(\n      instance=container(\"debian:latest\"),\n      instructions=[script(\"make\")]\n    ),\n  ]\n</code></pre> <p><code>main()</code> needs to return a list of task objects which will be serialized into YAML, like this:</p> <pre><code>task:\n    container:\n      image: debian:latest\n    script: make\n</code></pre> <p>Then the generated YAML is appended to <code>.cirrus.yml</code> (if any) before passing the combined config into the final YAML parser.</p> <p>With Starlark, it's possible to generate parts of the configuration dynamically based on some external conditions:</p> <ul> <li>Parsing files inside the repository to pick up some common settings (for example, parse <code>package.json</code> to see if it contains <code>lint</code> script and generate a linting task).</li> <li>Making an HTTP request to check the previous build status.</li> </ul> <p>See a video tutorial on how to create a custom Cirrus module:</p>","title":"Writing Starlark scripts"},{"location":"guide/programming-tasks/#entrypoints","text":"<p>Different events will trigger execution of different top-level functions in the <code>.cirrus.star</code>. These functions reserve certain names and will be called with different arguments depending on the event which triggered the execution.</p>","title":"Entrypoints"},{"location":"guide/programming-tasks/#main","text":"<p><code>main()</code> is called once a Cirrus CI build is triggered in order to generate a list of tasks to execute within that particular build:</p> <pre><code>def main():\n    return [\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make test\"\n      },\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make build\"\n      }\n    ]\n</code></pre> <p>If you want to return multiple tasks with the same name or a top-level override like <code>env</code>, use the tuple syntax below:</p> <pre><code>def main():\n    return [\n      (\"env\", {\"PARALLEL\": \"yes\"}),\n      (\"container\", {\"image\": \"debian:latest\"}),\n      (\"task\", {\"script\": \"make build\"}),\n      (\"task\", {\"script\": \"make test\"})\n    ]\n</code></pre> <p>In regard to top-level overrides, note that when using both YAML and Starlark configuration formats they get merged and the YAML configuration always comes first.</p>","title":"<code>main()</code>"},{"location":"guide/programming-tasks/#hooks","text":"<p>It's also possible to execute Starlark scripts on updates to the current build or any of the tasks within the build. Think of it as WebHooks running within Cirrus that doesn't require any infrastructure on your end.</p> <p>Expected names of Starlark Hook functions in <code>.cirrus.star</code> are <code>on_build_&lt;STATUS&gt;</code> or <code>on_task_&lt;STATUS&gt;</code> respectively. Please refer to Cirrus CI GraphQL Schema for a full list of existing statuses, but most commonly  <code>on_build_failed</code>/<code>on_build_completed</code> and <code>on_task_failed</code>/<code>on_task_completed</code> are used. These functions should expect a single context argument passed by Cirrus Cloud. At the moment hook's context only contains a single field <code>payload</code> containing the same payload as a webhook. </p> <p>One caveat of Starlark Hooks execution is <code>CIRRUS_TOKEN</code> environment variable that contains a token to access Cirrus API. Scope of <code>CIRRUS_TOKEN</code> is restricted to the build associated with that particular hook invocation and allows, for example, to automatically re-run tasks. Here is an example of a Starlark Hook that automatically re-runs a failed task in case a particular transient issue found in logs:</p> <pre><code># load some helpers from an external module \nload(\"github.com/cirrus-modules/graphql\", \"rerun_task_if_issue_in_logs\")\n\ndef on_task_failed(ctx):\n  if \"Test\" not in ctx.payload.data.task.name:\n    return\n  if ctx.payload.data.task.automaticReRun:\n    print(\"Task is already an automatic re-run! Won't even try to re-run it...\")\n    return\n  rerun_task_if_issue_in_logs(ctx.payload.data.task.id, \"Time out\")\n</code></pre>","title":"Hooks"},{"location":"guide/programming-tasks/#module-loading","text":"<p>Module loading is done through the Starlark's <code>load()</code> statement.</p> <p>Besides the ability to load builtins with it, Cirrus can load other <code>.star</code> files from local and remote locations to facilitate code re-use.</p>","title":"Module loading"},{"location":"guide/programming-tasks/#local","text":"<p>Local loads are relative to the project's root (where <code>.cirrus.star</code> is located):</p> <pre><code>load(\".ci/notify-slack.star\", \"notify_slack\")\n</code></pre>","title":"Local"},{"location":"guide/programming-tasks/#remote-from-git","text":"<p>To load the default branch of the module from GitHub:</p> <pre><code>load(\"github.com/cirrus-modules/golang\", \"task\", \"container\")\n</code></pre> <p>In the example above, the name of the <code>.star</code> file was not provided, because <code>lib.star</code> is assumed by default. This is equivalent to:</p> <pre><code>load(\"github.com/cirrus-modules/golang/lib.star@main\", \"task\", \"container\")\n</code></pre> <p>You can also specify an exact commit hash instead of the <code>main()</code> branch name to prevent accidental changes.</p>  <p>Loading private modules</p> <p>If your organization has private repository called <code>cirrus-modules</code> with installed Cirrus CI, then this repository will be available for loading within repositories of your organization.</p>  <p>To load <code>.star</code> files from repositories other than GitHub, add a <code>.git</code> suffix at the end of the repository name, for example:</p> <pre><code>load(\"gitlab.com/fictional/repository.git/validator.star\", \"validate\")\n                                     ^^^^ note the suffix\n</code></pre>","title":"Remote from Git"},{"location":"guide/programming-tasks/#builtins","text":"<p>Cirrus CLI provides builtins all nested in the <code>cirrus</code> module that greatly extend what can be done with the Starlark alone.</p>","title":"Builtins"},{"location":"guide/programming-tasks/#fs","text":"<p>These builtins allow for read-only filesystem access.</p> <p>All paths are relative to the project's directory.</p>","title":"<code>fs</code>"},{"location":"guide/programming-tasks/#fsexistspath","text":"<p>Returns <code>True</code> if <code>path</code> exists and <code>False</code> otherwise.</p>","title":"<code>fs.exists(path)</code>"},{"location":"guide/programming-tasks/#fsisdirpath","text":"<p>Returns <code>True</code> if <code>path</code> points to a directory and <code>False</code> otherwise.</p>","title":"<code>fs.isdir(path)</code>"},{"location":"guide/programming-tasks/#fsreadpath","text":"<p>Returns a <code>string</code> with the file contents or <code>None</code> if the file doesn't exist.</p> <p>Note that this is an error to read a directory with <code>fs.read()</code>.</p>","title":"<code>fs.read(path)</code>"},{"location":"guide/programming-tasks/#fsreaddirdirpath","text":"<p>Returns a <code>list</code> of <code>string</code>'s with names of the entries in the directory or <code>None</code> if the directory does not exist.</p> <p>Note that this is an error to read a file with <code>fs.readdir()</code>.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if fs.exists(\"go.mod\"):\n        tasks += go_tasks()\n\n    return tasks\n</code></pre>","title":"<code>fs.readdir(dirpath)</code>"},{"location":"guide/programming-tasks/#is_test","text":"<p>While not technically a builtin, <code>is_test</code> is a <code>bool</code> that allows Starlark code to determine whether it's running in test environment via Cirrus CLI. This can be useful for limiting the test complexity, e.g. by not making a real HTTP request and mocking/skipping it instead. Read more about module testing in a separate guide in Cirrus CLI repository.</p>","title":"<code>is_test</code>"},{"location":"guide/programming-tasks/#env","text":"<p>While not technically a builtin, <code>env</code> is dict that contains environment variables.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"env\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if env.get(\"CIRRUS_TAG\") != None:\n        tasks += release_tasks()\n\n    return tasks\n</code></pre>","title":"<code>env</code>"},{"location":"guide/programming-tasks/#changes_include","text":"<p><code>changes_include()</code> is a Starlark alternative to the changesInclude() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched any of the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if changes_include(\"Dockerfile\"):\n        tasks += docker_task()\n\n    return tasks\n</code></pre>","title":"<code>changes_include</code>"},{"location":"guide/programming-tasks/#changes_include_only","text":"<p><code>changes_include_only()</code> is a Starlark alternative to the changesIncludeOnly() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched all the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include_only\")\n\ndef main(ctx):\n    if changes_include_only(\"doc/*\"):\n        return []\n\n    return base_tasks()\n</code></pre>","title":"<code>changes_include_only</code>"},{"location":"guide/programming-tasks/#http","text":"<p>Provides HTTP client implementation with <code>http.get()</code>, <code>http.post()</code> and other HTTP method functions.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>http</code>"},{"location":"guide/programming-tasks/#hash","text":"<p>Provides cryptographic hashing functions, such as <code>hash.md5()</code>, <code>hash.sha1()</code> and <code>hash.sha256()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>hash</code>"},{"location":"guide/programming-tasks/#base64","text":"<p>Provides Base64 encoding and decoding functions using <code>base64.encode()</code> and <code>base64.decode()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>base64</code>"},{"location":"guide/programming-tasks/#json","text":"<p>Provides JSON document marshalling and unmarshalling using <code>json.dumps()</code> and <code>json.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>json</code>"},{"location":"guide/programming-tasks/#yaml","text":"<p>Provides YAML document marshalling and unmarshalling using <code>yaml.dumps()</code> and <code>yaml.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>yaml</code>"},{"location":"guide/programming-tasks/#re","text":"<p>Provides regular expression functions, such as <code>findall()</code>, <code>split()</code> and <code>sub()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","title":"<code>re</code>"},{"location":"guide/programming-tasks/#zipfile","text":"<p><code>cirrus.zipfile</code> module provides methods to read Zip archives.</p> <p>You instantiate a <code>ZipFile</code> object using <code>zipfile.ZipFile(data)</code> function call and then call <code>namelist()</code> and <code>open(filename)</code> methods to retrieve information about archive contents.</p> <p>Refer to the starlib's documentation for more details.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\", \"zipfile\")\n\ndef is_java_archive(path):\n    # Read Zip archive contents from the filesystem\n    archive_contents = fs.read(path)\n    if archive_contents == None:\n        return False\n\n    # Open Zip archive and a file inside of it\n    zf = zipfile.ZipFile(archive_contents)\n    manifest = zf.open(\"META-INF/MANIFEST.MF\")\n\n    # Does the manifest contain the expected version?\n    if \"Manifest-Version: 1.0\" in manifest.read():\n        return True\n\n    return False\n</code></pre>","title":"<code>zipfile</code>"},{"location":"guide/quick-start/","text":"<p>At the moment Cirrus CI only supports repositories hosted on GitHub. This guide will walk you through the installation process. If you are interested in a support for other code hosting platforms please fill up this form to help us prioritize the support and notify you once the support is available.</p> <p>Start by configuring the Cirrus CI application from GitHub Marketplace.</p> <p></p> <p>Choose a plan for your personal account or for an organization you have admin writes for.</p> <p></p> <p>GitHub Apps can be installed on all repositories or on repository-by-repository basis for granular access control. For example, Cirrus CI can be installed only on public repositories and will only have access to these public repositories. In contrast, classic OAuth Apps don't have such restrictions.</p> <p></p>  <p>Change Repository Access</p> <p>You can always revisit Cirrus CI's repository access settings on your installation page.</p>","title":"Quick Start"},{"location":"guide/quick-start/#post-installation","text":"<p>Once Cirrus CI is installed for a particular repository, you must add either <code>.cirrus.yml</code> configuration or <code>.cirrus.star</code> script to the root of the repository.  The <code>.cirrus.yml</code> defines tasks that will be executed for every build for the repository. </p> <p>For a Node.js project, your <code>.cirrus.yml</code> could look like:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre>    <p>That's all! After pushing a <code>.cirrus.yml</code> a build with all the tasks defined in the <code>.cirrus.yml</code> file will be created.</p> <p>Note: Please check the full guide on configuring Cirrus Tasks and/or check a list of available examples.</p>  <p>Zero-config Docker Builds</p> <p>If your repository happened to have a <code>Dockerfile</code> in the root, Cirrus CI will attempt to build it even without a corresponding <code>.cirrus.yml</code> configuration file.</p>  <p>You will see all your Cirrus CI builds on cirrus-ci.com once signed in. </p> <p></p> <p>GitHub status checks for each task will appear on GitHub as well.</p> <p></p> <p>Newly created PRs will also get Cirrus CI's status checks.</p> <p></p>  <p>Examples</p> <p>Don't forget to check examples page for ready-to-copy examples of some <code>.cirrus.yml</code>  configuration files for different languages and build systems.</p>   <p>Life of a build</p> <p>Please check a high level overview of what's happening under the hood when a changed is pushed and this guide to learn more about how to write tasks.</p>","title":"Post Installation"},{"location":"guide/quick-start/#authorization-on-cirrus-ci-web-app","text":"<p>All builds created by your account can be viewed on Cirrus CI Web App after signing in with your GitHub Account:</p> <p></p> <p>After clicking on <code>Sign In</code> you'll be redirected to GitHub in order to authorize access:</p> <p></p>  <p>Note about Act on your behalf</p> <p>Cirrus CI only asks for several kinds of permissions that you can see on your installation page. These permissions are read-only except for write access to checks and commit statuses in order for Cirrus CI to be able to report task statuses via checks or commit statuses.</p> <p>There is a long thread disscussing this weird \"Act on your behalf\" wording here on GitHub's own commuity forum.</p>","title":"Authorization on Cirrus CI Web App"},{"location":"guide/quick-start/#enabling-new-repositories-after-installation","text":"<p>If you choose initially to allow Cirrus CI to access all of your repositories, all you need to do is push a <code>.cirrus.yml</code> to start building your repository on Cirrus CI.</p> <p>If you only allowed Cirrus CI to access certain repositories, then add your new repository to the list of repositories Cirrus CI has access to via this page, then push a <code>.cirrus.yml</code> to start building on Cirrus CI.</p>","title":"Enabling New Repositories after Installation"},{"location":"guide/supported-computing-services/","text":"<p>             </p> <p>          </p> <p>       </p> <p>       </p> <p>For every task Cirrus CI starts a new Virtual Machine or a new Docker Container on a given compute service. Using a new VM or a new Docker Container each time for running tasks has many benefits:</p> <ul> <li>Atomic changes to an environment where tasks are executed. Everything about a task is configured in <code>.cirrus.yml</code> file, including   VM image version and Docker Container image version. After committing changes to <code>.cirrus.yml</code> not only new tasks will use the new environment,   but also outdated branches will continue using the old configuration.</li> <li>Reproducibility. Fresh environment guarantees no corrupted artifacts or caches are presented from the previous tasks.</li> <li>Cost efficiency. Most compute services are offering per-second pricing which makes them ideal for using with Cirrus CI.    Also each task for repository can define ideal amount of CPUs and Memory specific for a nature of the task. No need to manage   pools of similar VMs or try to fit workloads within limits of a given Continuous Integration systems.</li> </ul> <p>To be fair there are of course some disadvantages of starting a new VM or a container for every task:</p> <ul> <li>Virtual Machine Startup Speed. Starting a VM can take from a few dozen seconds to a minute or two depending on a cloud provider and   a particular VM image. Starting a container on the other hand just takes a few hundred milliseconds! But even a minute   on average for starting up VMs is not a big inconvenience in favor of more stable, reliable and more reproducible CI.</li> <li>Cold local caches for every task execution. Many tools tend to store some caches like downloaded dependencies locally   to avoid downloading them again in future. Since Cirrus CI always uses fresh VMs and containers such local caches will always   be empty. Performance implication of empty local caches can be avoided by using Cirrus CI features like    built-in caching mechanism. Some tools like Gradle can    even take advantages of built-in HTTP cache!</li> </ul> <p>Please check the list of currently supported cloud compute services below. In case you have your own hardware, please take a look at Persistent Workers, which allow connecting anything to Cirrus CI.</p>","title":"Computing Services"},{"location":"guide/supported-computing-services/#google-cloud","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several Google Cloud Compute services. In order to interact with Google Cloud APIs  Cirrus CI needs permissions. Creating a service account  is a common way to safely give granular access to parts of Google Cloud Projects. </p>  <p>Isolation</p> <p>We do recommend to create a separate Google Cloud project for running CI builds to make sure tests are isolated from production data. Having a separate project also will show how much money is spent on CI and how efficient Cirrus CI is </p>  <p>Once you have a Google Cloud project for Cirrus CI please create a service account by running the following command: </p> <pre><code>gcloud iam service-accounts create cirrus-ci \\\n    --project $PROJECT_ID\n</code></pre> <p>Depending on a compute service Cirrus CI will need different roles  assigned to the service account. But Cirrus CI will always need permissions to act as a service account and be able to view monitoring:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/iam.serviceAccountUser \\\n    --role roles/monitoring.viewer\n</code></pre> <p>Cirrus CI uses Google Cloud Storage to store logs and caches. In order to give Google Cloud Storage permissions to the service account please run:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/storage.admin\n</code></pre>  <p>Default Logs Retentions Period</p> <p>By default Cirrus CI will store logs and caches for 90 days but it can be changed by manually configuring a lifecycle rule for a Google Cloud Storage bucket that Cirrus CI is using.</p>  <p>Now we have a service account that Cirrus CI can use! It's time to let Cirrus CI know about that fact by securely providing a private key for the service account. A private key can be created by running the following command:</p> <pre><code>gcloud iam service-accounts keys create service-account-credentials.json \\\n  --iam-account cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com\n</code></pre> <p>At last create an encrypted variable from contents of <code>service-account-credentials.json</code> file and add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>Now Cirrus CI can store logs and caches in Google Cloud Storage for tasks scheduled on either GCE or GKE. Please check following sections  with additional instructions about Compute Engine or Kubernetes Engine.</p>  <p>Supported Regions</p> <p>Cirrus CI currently supports following GCP regions: <code>us-central1</code>, <code>us-east1</code>, <code>us-east4</code>, <code>us-west1</code>, <code>us-west2</code>, <code>europe-west1</code>, <code>europe-west2</code>, <code>europe-west3</code> and <code>europe-west4</code>.</p> <p>Please contact support if you are interested in support for other regions.</p>","title":"Google Cloud"},{"location":"guide/supported-computing-services/#compute-engine","text":"<p>    </p> <p>In order to schedule tasks on Google Compute Engine a service account that Cirrus CI operates via should have a necessary role assigned. It can be done by running a <code>gcloud</code> command:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/compute.admin\n</code></pre> <p>Now tasks can be scheduled on Compute Engine within <code>$PROJECT_ID</code> project by configuring <code>gce_instance</code> something  like this:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_name: ubuntu-1904-disco-v20190417\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 60\n  use_ssd: true # default to false\n\ntask:\n  script: ./run-ci.sh\n</code></pre>  <p>Specify Machine Type</p> <p>It is possible to specify a predefined machine type via <code>type</code> field:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_name: ubuntu-1604-xenial-v20171121a\n  zone: us-central1-a\n  type: n1-standard-8\n  disk: 20\n</code></pre>   <p>Specify Image Family</p> <p>It's also possible to specify image family instead of the concrete image name. Use the <code>image_family</code> field instead of <code>image_name</code>:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_family: ubuntu-1904\n</code></pre>","title":"Compute Engine"},{"location":"guide/supported-computing-services/#custom-vm-images","text":"<p>Building an immutable VM image with all necessary software pre-configured is a known best practice with many benefits. It makes sure environment where a task is executed is always the same and that no time is spent on useless work like installing a package over and over again for every single task.</p> <p>There are many ways how one can create a custom image for Google Compute Engine. Please refer to the official documentation. At Cirrus Labs we are using Packer to automate building such images. An example of how we use it can be found in our public GitHub repository.</p>","title":"Custom VM images"},{"location":"guide/supported-computing-services/#windows-support","text":"<p>Google Compute Engine support Windows images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: windows-cloud\n  image_name: windows-server-2016-dc-core-v20170913\n  platform: windows\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntask:\n  script: run-ci.bat\n</code></pre>","title":"Windows Support"},{"location":"guide/supported-computing-services/#freebsd-support","text":"<p>Google Compute Engine support FreeBSD images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: freebsd-org-cloud-dev\n  image_family: freebsd-12-1\n  platform: FreeBSD\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 50\n\ntask:\n  script: printenv\n</code></pre>","title":"FreeBSD Support"},{"location":"guide/supported-computing-services/#docker-containers-on-dedicated-vms","text":"<p>It is possible to run a container directly on a Compute Engine VM with pre-installed Docker. Use the <code>gce_container</code> field to specify a VM image and a Docker container to execute on the VM (<code>gce_container</code> extends <code>gce_instance</code> definition with a few additional fields):</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker\n  container: golang:latest\n  additional_containers:\n    - name: redis\n      image: redis:3.2-alpine\n      port: 6379\n</code></pre> <p>Note that <code>gce_container</code> always runs containers in privileged mode.</p> <p>If your VM image has Nested Virtualization Enabled it's possible to use KVM from the container by specifying <code>enable_nested_virtualization</code> flag. Here is an example of using KVM-enabled container to run a hardware accelerated Android emulator:</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker-and-KVM\n  container: cirrusci/android-sdk:29\n  enable_nested_virtualization: true\n  accel_check_script:\n    - sudo chown cirrus:cirrus /dev/kvm\n    - emulator -accel-check\n</code></pre>","title":"Docker Containers on Dedicated VMs"},{"location":"guide/supported-computing-services/#instance-scopes","text":"<p>By default Cirrus CI will create Google Compute instances without any scopes  so an instance can't access Google Cloud Storage for example. But sometimes it can be useful to give some permissions to an  instance by using <code>scopes</code> key of <code>gce_instance</code>.  For example, if a particular task builds Docker images and then pushes  them to Container Registry, its configuration file can look something like:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntest_task:\n  test_script: ./scripts/test.sh\n\npush_docker_task:\n  depends_on: test\n  only_if: $CIRRUS_BRANCH == \"master\"\n  gce_instance:\n    scopes: cloud-platform\n  push_script: ./scripts/push_docker.sh\n</code></pre>","title":"Instance Scopes"},{"location":"guide/supported-computing-services/#preemptible-instances","text":"<p>Cirrus CI can schedule preemptible instances with all price benefits and stability risks. But sometimes risks of an instance being preempted at any time can be tolerated. For example  <code>gce_instance</code> can be configured to schedule preemptible instance for non master branches like this:</p> <pre><code>gce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  preemptible: $CIRRUS_BRANCH != \"master\"\n</code></pre>","title":"Preemptible Instances"},{"location":"guide/supported-computing-services/#kubernetes-engine","text":"<p>    </p> <p>Scheduling tasks on Compute Engine has one big disadvantage of waiting for an instance to start which usually takes around a minute. One minute is not that long but can't compete with hundreds of milliseconds that takes a container cluster on GKE to start a container.</p> <p>To start scheduling tasks on a container cluster we first need to create one using <code>gcloud</code>. Here is a recommended configuration of a cluster that is very similar to what is used for the managed <code>contianer</code> instances. We recommend creating a cluster with two node pools:</p> <ul> <li><code>default-pool</code> with a single node and no autoscaling for system pods required by Kubernetes.</li> <li><code>workers-pool</code> that will use Compute-Optimized instances   and SSD storage for better performance. This pool also will be able to scale to 0 when there are no tasks to run.</li> </ul> <pre><code>gcloud container clusters create cirrus-ci-cluster \\\n  --autoscaling-profile optimize-utilization \\\n  --zone us-central1-a \\\n  --num-nodes \"1\" \\\n  --machine-type \"e2-standard-2\" \\\n  --disk-type \"pd-standard\" --disk-size \"100\"\n\ngcloud container node-pools create \"workers-pool\" \\\n  --cluster cirrus-ci-cluster \\\n  --zone \"us-central1-a\" \\\n  --num-nodes \"0\" \\\n  --enable-autoscaling --min-nodes \"0\" --max-nodes \"8\" \\\n  --node-taints dedicated=system:PreferNoSchedule \\\n  --machine-type \"c2-standard-30\" \\\n  --disk-type \"pd-ssd\" --disk-size \"500\"\n</code></pre> <p>A service account that Cirrus CI operates via should be assigned with <code>container.admin</code> role that allows to administrate GKE clusters:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/container.admin\n</code></pre> <p>Done! Now after creating <code>cirrus-ci-cluster</code> cluster and having <code>gcp_credentials</code> configured tasks can be scheduled on the  newly created cluster like this:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngke_container:\n  image: gradle:jdk8\n  cluster_name: cirrus-ci-cluster\n  location: us-central1-a # cluster zone or region for multi-zone clusters\n  namespace: default # Kubernetes namespace to create pods in\n  cpu: 6\n  memory: 24GB\n</code></pre>  <p>Using in-memory disk</p> <p>By default Cirrus CI mounts an emptyDir into  <code>/tmp</code> path to protect the pod from unnecessary eviction by autoscaler. It is possible to switch emptyDir's medium to  use in-memory <code>tmpfs</code> storage instead of a default one by setting <code>use_in_memory_disk</code> field of <code>gke_container</code> to <code>true</code> or any other expression that uses environment variables.</p>   <p>Running privileged containers</p> <p>You can run privileged containers on your private GKE cluster by setting <code>privileged</code> field of <code>gke_container</code> to <code>true</code>  or any other expression that uses environment variables. <code>privileged</code> field is also available for any additional container.</p> <p>Here is an example of how to run docker-in-docker</p> <pre><code>gke_container:\n  image: my-docker-client:latest\n  cluster_name: my-gke-cluster\n  location: us-west1-c\n  namespace: cirrus-ci\n  additional_containers:\n    - name: docker\n      image: docker:dind\n      privileged: true\n      cpu: 2\n      memory: 6G\n      port: 2375\n</code></pre>  <p>For a full example on leveraging this to do docker-in-docker builds on Kubernetes checkout Docker Builds on Kubernetes</p>  Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","title":"Kubernetes Engine"},{"location":"guide/supported-computing-services/#aws","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several AWS services. In order to interact with AWS APIs Cirrus CI needs permissions.  Creating an IAM user for programmatic access is a common way to safely give granular access to parts of your AWS.</p> <p>Once you created a user for Cirrus CI you'll need to provide key id and access key itself. In order to do so please create an encrypted variable with the following content:</p> <pre><code>[default]\naws_access_key_id=...\naws_secret_access_key=...\n</code></pre> <p>Then you'll be able to use the encrypted variable in your <code>.cirrus.yml</code> file like this:</p> <pre><code>aws_credentials: ENCRYPTED[...]\n\ntask:  \n  ec2_instance:\n    ...\n\ntask:  \n  eks_instance:\n    ...\n</code></pre>  <p>Permissions</p> <p>A user that Cirrus CI will be using for orchestrating tasks on AWS should at least have access to S3 in order to store logs and cache artifacts. Here is a list of actions that Cirrus CI requires to store logs and artifacts:</p> <pre><code>\"Action\": [\n  \"s3:CreateBucket\",\n  \"s3:GetObject\",\n  \"s3:PutObject\",\n  \"s3:DeleteObject\",\n  \"s3:PutLifecycleConfiguration\"\n]\n</code></pre>","title":"AWS"},{"location":"guide/supported-computing-services/#ec2","text":"<p>     </p> <p>In order to schedule tasks on EC2 please make sure that IAM user that Cirrus CI is using has following permissions:</p> <pre><code>\"Action\": [\n  \"ec2:DescribeInstances\",\n  \"ec2:RunInstances\",\n  \"ec2:TerminateInstances\"\n]\n</code></pre> <p>Now tasks can be scheduled on EC2 by configuring <code>ec2_instance</code> something like this:</p> <pre><code>task:\n  ec2_instance:\n    image: ami-03790f6959fc34ef3\n    type: t2.micro\n    region: us-east-1\n  script: ./run-ci.sh\n</code></pre>","title":"EC2"},{"location":"guide/supported-computing-services/#eks","text":"<p>     </p> <p>Please follow instructions on how to create a EKS cluster and add workers nodes to it. And don't forget to add necessary permissions for the IAM user that Cirrus CI is using:</p> <pre><code>\"Action\": [\n  \"iam:PassRole\",\n  \"eks:DescribeCluster\",\n  \"eks:CreateCluster\",\n  \"eks:DeleteCluster\",\n  \"eks:UpdateClusterVersion\"\n]\n</code></pre> <p>To verify that Cirrus CI will be able to communicate with your cluster please make sure that if you are locally logged in as the user that Cirrus CI acts as you can successfully run the following commands and see your worker nodes up and running:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION update-kubeconfig --name $CLUSTER_NAME\n$: kubectl get nodes\n</code></pre>  <p>EKS Access Denied</p> <p>If you have an issue with accessing your EKS cluster via <code>kubectl</code>, most likely you did not create the cluster with the user that Cirrus CI is using. The easiest way to do so is to create the cluster through AWS CLI with the following command:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION \\\n    create-cluster --name cirrus-ci \\\n    --role-arn ... \\\n    --resources-vpc-config subnetIds=...,securityGroupIds=...\n</code></pre>  <p>Now tasks can be scheduled on EKS by configuring <code>eks_container</code> something like this:</p> <pre><code>task:\n  eks_container:\n    image: node:latest\n    region: us-east-1\n    cluster_name: cirrus-ci\n  script: ./run-ci.sh\n</code></pre>  <p>S3 Access for Caching</p> <p>Please add <code>AmazonS3FullAccess</code> policy to the role used for creation of EKS workers (same role you put in <code>aws-auth-cm.yaml</code> when enabled worker nodes to join the cluster).</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","title":"EKS"},{"location":"guide/supported-computing-services/#azure","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several Azure services. In order to interact with Azure APIs  Cirrus CI needs permissions. First, please choose a subscription you want to use for scheduling CI tasks. Navigate to the Subscriptions blade within the Azure Portal and save <code>$SUBSCRIPTION_ID</code> that we'll use below for setting up a service principle.</p> <p>Creating a service principal  is a common way to safely give granular access to parts of Azure:</p> <pre><code>az ad sp create-for-rbac --name CirrusCI --sdk-auth \\\n  --scopes \"/subscriptions/$SUBSCRIPTION_ID\"\n</code></pre> <p>Command above will create a new service principal and will print something like:</p> <pre><code>{\n  \"clientId\": \"...\",\n  \"clientSecret\": \"...\",\n  \"subscriptionId\": \"...\",\n  \"tenantId\": \"...\",\n  ...\n}\n</code></pre> <p>Please also remember <code>clientId</code> from the JSON as <code>$CIRRUS_CLIENT_ID</code>. It will be used later for configuring blob storage access.</p> <p>Please create an encrypted variable from this output and  add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>azure_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>You also need to create a resource group that Cirrus CI will use for scheduling tasks:</p> <pre><code>az group create --location eastus --name CirrusCI\n</code></pre> <p>Please also allow the newly created CirrusCI principle to access blob storage in order to manage logs and caches.</p> <pre><code>az role assignment create \\\n    --role \"Storage Blob Data Contributor\" \\\n    --assignee $CIRRUS_CLIENT_ID \\\n    --scope \"/subscriptions/$SUBSCRIPTION_ID/resourceGroups/CirrusCI\"\n</code></pre> <p>Now Cirrus CI can interact with Azure APIs.</p>","title":"Azure"},{"location":"guide/supported-computing-services/#azure-container-instances","text":"<p>    </p> <p>Azure Container Instances (ACI) is an ideal  candidate for running modern CI workloads. ACI allows just to run Linux and Windows containers without thinking about  underlying infrastructure.</p> <p>Once <code>azure_credentials</code> is configured as described above, tasks can be scheduled on ACI by configuring <code>aci_instance</code> like this:</p> <pre><code>azure_container_instance:\n  image: cirrusci/windowsservercore:2016\n  resource_group: CirrusCI\n  region: westus\n  platform: windows\n  cpu: 4\n  memory: 12G\n</code></pre>  <p>About Docker Images to use with ACI</p> <p>Linux-based images are usually pretty small and doesn't require much tweaking. For Windows containers ACI recommends to follow a few basic tips in order to reduce startup time.</p>","title":"Azure Container Instances"},{"location":"guide/supported-computing-services/#oracle-cloud","text":"<p>    </p> <p>Cirrus CI can schedule tasks on several Oracle Cloud services. In order to interact with OCI APIs Cirrus CI needs permissions. Please create a user that Cirrus CI will behalf on:</p> <pre><code>oci iam user create --name cirrus --description \"Cirrus CI Orchestrator\"\n</code></pre> <p>Please configure the <code>cirrus</code> user to be able to access storage, launch instances and have access to Kubernetes clusters. The easiest way is to add <code>cirrus</code> user to <code>Administrators</code> group, but it's not as secure as a granular access configuration.</p> <p>By default, for every repository you'll start using Cirrus CI with, Cirrus will create a bucket with 90 days lifetime policy. In order to allow Cirrus to configure lifecycle policies please add the following policy as described in the documentation. Here is an example of the policy for <code>us-ashburn-1</code> region:</p> <pre><code>Allow service objectstorage-us-ashburn-1 to manage object-family in tenancy\n</code></pre> <p>Once you created and configured <code>cirrus</code> user you'll need to provide its API key. Once you generate an API key you should get a <code>*.pem</code> file with the private key that will be used by Cirrus CI.</p> <p>Normally your config file for local use looks like this:</p> <pre><code>[DEFAULT]\nuser=ocid1.user.oc1..XXX\nfingerprint=11:22:...:99\ntenancy=ocid1.tenancy.oc1..YYY\nregion=us-ashburn-1\nkey_file=&lt;path to your *.pem private keyfile&gt;\n</code></pre> <p>For Cirrus to use, you'll need to use a different format:</p> <pre><code>&lt;user value&gt;\n&lt;fingerprint value&gt;\n&lt;tenancy value&gt;\n&lt;region value&gt;\n&lt;content of your *.pem private keyfile&gt;\n</code></pre> <p>This way you'll be able to create a single encrypted variable with the contents of the Cirrus specific credentials above.</p> <pre><code>oracle_credentials: ENCRYPTED[qwerty239abc]\n</code></pre>","title":"Oracle Cloud"},{"location":"guide/supported-computing-services/#kubernetes-cluster","text":"<p>    </p> <p>Please create a Kubernetes cluster and make sure Kubernetes API Public Endpoint is enabled for the cluster so Cirrus can access it. Then copy cluster id which can be used in configuring <code>oke_container</code>:</p> <pre><code>task:\n  oke_container:\n    cluster_id: ocid1.cluster.oc1.iad.xxxxxx\n    image: golang:latest\n  script: ./run-ci.sh\n</code></pre>  <p>Ampere A1 Support</p> <p>The cluster can utilize Oracle's Ampere A1 Arm instances in order to run <code>arm64</code> CI workloads!</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","title":"Kubernetes Cluster"},{"location":"guide/tips-and-tricks/","text":"","title":"Configuration Tips and Tricks"},{"location":"guide/tips-and-tricks/#custom-clone-command","text":"<p>By default, Cirrus CI uses a Git client implemented purely in Go to perform a clone of a single branch with full Git history. It is possible to control clone depth via <code>CIRRUS_CLONE_DEPTH</code> environment variable.</p> <p>Customizing clone behavior is a simple as overriding <code>clone_script</code>. For example, here an override to use a pre-installed Git client (if your build environment has it) to do a shallow clone of a single branch:</p> <pre><code>task:\n  clone_script: |\n    if [ -z \"$CIRRUS_PR\" ]; then\n      git clone --recursive --branch=$CIRRUS_BRANCH https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    else\n      git clone --recursive https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git fetch origin pull/$CIRRUS_PR/head:pull/$CIRRUS_PR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    fi\n  # ...\n</code></pre>  <p><code>go-git</code> benefits</p> <p>Using <code>go-git</code> made it possible not to require a pre-installed Git from an execution environment. For example, most of <code>alpine</code>-based containers don't have Git pre-installed. Because of <code>go-git</code> you can even use distroless containers with Cirrus CI, which don't even have an Operating System.</p>","title":"Custom Clone Command"},{"location":"guide/tips-and-tricks/#sharing-configuration-between-tasks","text":"<p>You can use YAML aliases to share configuration options between multiple tasks. For example, here is a 2-task build which only runs for \"master\", PRs and tags, and installs some framework:</p> <pre><code># Define a node anywhere in YAML file to create an alias. Make sure the name doesn't clash with an existing keyword.\nregular_task_template: &amp;REGULAR_TASK_TEMPLATE\n  only_if: $CIRRUS_BRANCH == 'master' || $CIRRUS_TAG != '' || $CIRRUS_PR != ''\n  env:\n    FRAMEWORK_PATH: \"${HOME}/framework\"\n  install_framework_script: curl https://example.com/framework.tar | tar -C \"${FRAMEWORK_PATH}\" -x\n\ntask:\n  # This operator will insert REGULAR_TASK_TEMPLATE at this point in the task node.\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: linux\n  container:\n    image: alpine:latest\n  test_script: ls \"${FRAMEWORK_PATH}\"\n\ntask:\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: osx\n  macos_instance:\n    image: catalina-xcode\n  test_script: ls -w \"${FRAMEWORK_PATH}\"\n</code></pre>","title":"Sharing configuration between tasks"},{"location":"guide/tips-and-tricks/#long-lines-in-configuration-file","text":"<p>If you like your YAML file to fit on your screen, and some commands are just too long, you can split them across multiple lines. YAML supports a variety of options to do that, for example here's how you can split ENCRYPTED values:</p> <pre><code>  env:\n    GOOGLE_APPLICATION_CREDENTIALS_DATA: \"ENCRYPTED\\\n      [3287dbace8346dfbe98347d1954eca923487fd8ea7251983\\\n      cb6d5edabdf6fe5abd711238764cbd6efbde6236abd6f274]\"\n</code></pre>","title":"Long lines in configuration file"},{"location":"guide/tips-and-tricks/#setting-environment-variables-from-scripts","text":"<p>Even through most of the time you can configure environment variables via <code>env</code>, there are cases when a variable value is obtained only when the task is already running.</p> <p>Normally you'd use <code>export</code> for that, but since each script instruction is executed in a separate shell, the exported variables won't propagate to the next instruction.</p> <p>However, there's a simple solution: just write your variables in a <code>KEY=VALUE</code> format to the file referenced by the <code>CIRRUS_ENV</code> environment variable.</p> <p>Here's a simple example:</p> <pre><code>task:\n  get_date_script: echo \"MEMOIZED_DATE=$(date)\" &gt;&gt; $CIRRUS_ENV\n  show_date_script: echo $MEMOIZED_DATE\n</code></pre>","title":"Setting environment variables from scripts"},{"location":"guide/windows/","text":"","title":"Windows Containers"},{"location":"guide/windows/#windows-containers","text":"<p>It is possible to run Windows Containers like how one can run Linux containers on Windows Community Cluster.  To use Windows, add <code>windows_container</code> instead of <code>container</code> in <code>.cirrus.yml</code> files:</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\ntask:\n  script: ...\n</code></pre> <p>Cirrus CI will execute scripts instructions like Batch scripts.</p>","title":"Windows Containers"},{"location":"guide/windows/#os-versions","text":"<p>By default, Cirrus CI assumes that the container image's host OS is Windows Server 2019. You can specify <code>os_version</code> to override it. Cirrus CI supports most versions of Windows Containers, including: <code>1709</code>, <code>1803</code> and <code>2019</code>.</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\nwindows_task:\n  install_script: choco install -y ...\n  ...\n</code></pre>","title":"OS Versions"},{"location":"guide/windows/#powershell-support","text":"<p>By default Cirrus CI agent executed scripts using <code>cmd.exe</code>. It is possible to override default shell executor by providing <code>CIRRUS_SHELL</code> environment variable:</p> <pre><code>env:\n  CIRRUS_SHELL: powershell\n</code></pre> <p>It is also possible to use PowerShell scripts inline inside of a script instruction by prefixing it with <code>ps</code>:</p> <pre><code>windows_task:\n  script:\n    - ps: Get-Location\n</code></pre> <p><code>ps: COMMAND</code> is just a syntactic sugar which transforms it to:</p> <pre><code>powershell.exe -NoLogo -EncodedCommand base64(COMMAND)\n</code></pre>","title":"PowerShell support"},{"location":"guide/windows/#environment-variables","text":"<p>Some software installed with Chocolatey would update <code>PATH</code> environment variable in system settings and suggest using <code>refreshenv</code> to pull those changes into the current environment. Unfortunately, using <code>refreshenv</code> will overwrite any environment variables set in Cirrus CI configuration with system-configured defaults. We advise to make necessary changes using <code>env</code> and <code>environment</code> instead of using <code>refreshenv</code> command in scripts.</p>","title":"Environment Variables"},{"location":"guide/windows/#chocolatey","text":"<p>All <code>cirrusci/*</code> Windows containers like <code>cirrusci/windowsservercore:2016</code> have Chocolatey pre-installed. Chocolatey is a package manager for Windows which supports unattended installs of software, useful on headless machines.</p>","title":"Chocolatey"},{"location":"guide/writing-tasks/","text":"<p>A <code>task</code> defines a sequence of instructions to execute and an execution environment to execute these instructions in. Let's see a line-by-line example of a <code>.cirrus.yml</code> configuration file first:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre>    <p>The example above defines a single task that will be scheduled and executed on the Linux Community Cluster using the <code>gradle:jdk11</code> Docker image. Only one user-defined script instruction to run <code>gradle test</code> will be executed. Not that complex, right?</p> <p>Please read the topics below if you want better understand what's going on in a more complex <code>.cirrus.yml</code> configuration file, such as this:</p> amd64arm64   <pre><code>task:\n  container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol>   <pre><code>task:\n  arm_container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      arm_container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol>     <p>Task Naming</p> <p>To name a task one can use the <code>name</code> field. <code>foo_task</code> syntax is a syntactic sugar. Separate name field is very useful when you want to have a rich task name:</p> <pre><code>task:\n  name: Tests (macOS)\n  ...\n</code></pre> <p>Note: instructions within a task can only be named via a prefix (e.g. <code>test_script</code>).</p>   <p>Visual Task Creation for Beginners</p> <p>If you are just getting started and prefer a more visual way of creating tasks, there is a third-party Cirrus CI Configuration Builder for generating YAML config that might be helpful.</p>","title":"Writing Tasks"},{"location":"guide/writing-tasks/#execution-environment","text":"<p>In order to specify where to execute a particular task you can choose from a variety of options by defining one of the following fields for a <code>task</code>:</p>    Field Name Managed by Description     <code>container</code> us Linux Docker Container   <code>arm_container</code> us Linux Arm Docker Container   <code>windows_container</code> us Windows Docker Container   <code>docker_builder</code> us Full-fledged VM pre-configured for running Docker   <code>macos_instance</code> us macOS Virtual Machines   <code>freebsd_instance</code> us FreeBSD Virtual Machines   <code>compute_engine_instance</code> us Full-fledged custom VM   <code>persistent_worker</code> you Use any host on any platform and architecture   <code>gce_instance</code> you Linux, Windows and FreeBSD Virtual Machines in your GCP project   <code>gke_container</code> you Linux Docker Containers on private GKE cluster   <code>ec2_instance</code> you Linux Virtual Machines in your AWS   <code>eks_instance</code> you Linux Docker Containers on private EKS cluster   <code>azure_container_instance</code> you Linux and Windows Docker Container on Azure   <code>oke_instance</code> you Linux x86 and Arm Containers on Oracle Cloud","title":"Execution Environment"},{"location":"guide/writing-tasks/#supported-instructions","text":"<p>Each task is essentially a collection of instructions that are executed sequentially. The following instructions are supported:</p> <ul> <li><code>script</code> instruction to execute a script.</li> <li><code>background_script</code> instruction to execute a script in a background.</li> <li><code>cache</code> instruction to persist files between task runs.</li> <li><code>artifacts</code> instruction to store and expose files created via a task.</li> <li><code>file</code> instruction to create a file from an environment variable.</li> </ul>","title":"Supported Instructions"},{"location":"guide/writing-tasks/#script-instruction","text":"<p>A <code>script</code> instruction executes commands via <code>shell</code> on Unix or <code>batch</code> on Windows. A <code>script</code> instruction can be named by adding a name as a prefix. For example <code>test_script</code> or <code>my_very_specific_build_step_script</code>. Naming script instructions helps gather more granular information about task execution. Cirrus CI will use it in future to auto-detect performance regressions.</p> <p>Script commands can be specified as a single string value or a list of string values in a <code>.cirrus.yml</code> configuration file like in the example below:</p> <pre><code>check_task:\n  compile_script: gradle --parallel classes testClasses\n  check_script:\n    - echo \"Here comes more than one script!\"\n    - printenv\n    - gradle check\n</code></pre> <p>Note: Each script instruction is executed in a newly created process, therefore environment variables are not preserved between them.</p>","title":"Script Instruction"},{"location":"guide/writing-tasks/#background-script-instruction","text":"<p>A <code>background_script</code> instruction is absolutely the same as <code>script</code> instruction but Cirrus CI won't wait for the script to finish and will continue execution of further instructions.</p> <p>Background scripts can be useful when something needs to be executed in the background. For example, a database or some emulators. Traditionally the same effect is achieved by adding <code>&amp;</code> to a command like <code>$: command &amp;</code>. Problem here is that logs from <code>command</code> will be mixed into regular logs of the following commands. By using background scripts not only logs will be properly saved and displayed, but also <code>command</code> itself will be properly killed in the end of a task.</p> <p>Here is an example of how <code>background_script</code> instruction can be used to run an android emulator:</p> <pre><code>android_test_task:\n  start_emulator_background_script: emulator -avd test -no-audio -no-window\n  wait_for_emulator_to_boot_script: adb wait-for-device\n  test_script: gradle test\n</code></pre>","title":"Background Script Instruction"},{"location":"guide/writing-tasks/#cache-instruction","text":"<p>A <code>cache</code> instruction allows to persist a folder and reuse it during the next execution of the task. A <code>cache</code> instruction can be named the same way as <code>script</code> instruction.</p> <p>Here is an example:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre>    <p>Either <code>folder</code> or a <code>folders</code> field (with a list of folder paths) is required and they tell the agent which folder paths to cache.</p> <p>Folder paths should be generally relative to the working directory (e.g. <code>node_modules</code>), with the exception of when only a single folder specified. In this case, it can be also an absolute path (<code>/usr/local/bundle</code>).</p> <p>Folder paths can contain a \"glob\" pattern to cache multiple files/folders within a working directory (e.g. <code>**/node_modules</code> will cache every <code>node_modules</code> folder within the working directory).</p> <p>A <code>fingerprint_script</code> and <code>fingerprint_key</code> are optional fields that can specify either:</p> <ul> <li>a script, the output of which will be hashed and used as a key for the given cache:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_script: cat yarn.lock\n</code></pre> <ul> <li>a final cache key:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_key: 2038-01-20\n</code></pre> <p>These two fields are mutually exclusive. By default the task name is used as a fingerprint value.</p> <p>After the last <code>script</code> instruction for the task succeeds, Cirrus CI will calculate checksum of the cached folder (note that it's unrelated to <code>fingerprint_script</code> or <code>fingerprint_key</code> fields) and re-upload the cache if it finds any changes. To avoid a time-costly re-upload, remove volatile files from the cache (for example, in the last <code>script</code> instruction of a task).</p> <p><code>populate_script</code> is an optional field that can specify a script that will be executed to populate the cache. <code>populate_script</code> should create the <code>folder</code> if it doesn't exist before the <code>cache</code> instruction. If your dependencies are updated often, please pay attention to <code>fingerprint_script</code> and make sure it will produce different outputs for different versions of your dependency (ideally just print locked versions of dependencies).</p> <p><code>reupload_on_changes</code> is an optional field that can specify whether Cirrus Agent should check if  contents of cached <code>folder</code> have changed during task execution and re-upload a cache entry in case of any changes. If <code>reupload_on_changes</code> option is not set explicitly then it will be set to <code>false</code> if <code>fingerprint_script</code> or <code>fingerprint_key</code> is presented and <code>true</code> otherwise. Cirrus Agent will detect additions, deletions and modifications of any files under specified <code>folder</code>. All of the detected changes will be logged under <code>Upload '$CACHE_NAME' cache</code> instructions for easier debugging of cache invalidations.</p> <p>That means the only difference between the example above and below is that <code>yarn install</code> will always be executed in the example below where in the example above only when <code>yarn.lock</code> has changes.</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre>     <p>Caching for Pull Requests</p> <p>Tasks for PRs upload caches to a separate caching namespace to not interfere with caches used by other tasks. But such PR tasks can read all caches even from the main caching namespace for a repository.</p>   <p>Scope of cached artifacts</p> <p>Cache artifacts are shared between tasks, so two caches with the same name on e.g. Linux containers and macOS VMs will share the same set of files. This may introduce binary incompatibility between caches. To avoid that, add <code>echo $CIRRUS_OS</code> into <code>fingerprint_script</code> or use <code>$CIRRUS_OS</code> in <code>fingerprint_key</code>, which will distinguish caches based on OS.</p>","title":"Cache Instruction"},{"location":"guide/writing-tasks/#manual-cache-upload","text":"<p>Normally caches are uploaded at the end of the task execution. However, you can override the default behavior and upload them earlier.</p> <p>To do this, use the <code>upload_caches</code> instruction, which uploads a list of caches passed to it once executed:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre>    <p>Note that <code>pip</code> cache won't be uploaded in this example: using <code>upload_caches</code> disables the default behavior where all caches are automatically uploaded at the end of the task, so if you want to upload <code>pip</code> cache too, you'll have to either:</p> <ul> <li>extend the list of uploaded caches in the first <code>upload_caches</code> instruction</li> <li>insert a second <code>upload_caches</code> instruction that specifically targets <code>pip</code> cache</li> </ul>","title":"Manual cache upload"},{"location":"guide/writing-tasks/#artifacts-instruction","text":"<p>An <code>artifacts</code> instruction allows to store files and expose them in the UI for downloading later. An <code>artifacts</code> instruction can be named the same way as <code>script</code> instruction and has only one required <code>path</code> field which accepts a glob pattern of files relative to <code>$CIRRUS_WORKING_DIR</code> to store. Right now only storing files under <code>$CIRRUS_WORKING_DIR</code> folder as artifacts is supported with a total size limit of 1G for a community task and with no limit on your own infrastructure.</p> <p>In the example below, Build and Test task produces two artifacts: <code>binaries</code> artifacts with all executables built during a successful task completion and <code>junit</code> artifacts with all test reports regardless of the final task status (more about that you can learn in the next section describing execution behavior).</p> <pre><code>build_and_test_task:\n  # instructions to build and test\n  binaries_artifacts:\n    path: \"build/*\"\n  always:\n    junit_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n</code></pre>  URLs to the artifacts","title":"Artifacts Instruction"},{"location":"guide/writing-tasks/#latest-build-artifacts","text":"<p>It is possible to refer to the latest artifacts directly (artifacts of the latest successful build). Use the following link format to download the latest artifact of a particular task:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;\n</code></pre> <p>It is possible to also download an archive of all files within an artifact with the following link:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>By default, Cirrus looks up the latest successful build of the default branch for the repository but the branch name can be customized via <code>?branch=&lt;BRANCH&gt;</code> query parameter.</p>","title":"Latest build artifacts"},{"location":"guide/writing-tasks/#current-build-artifacts","text":"<p>It is possible to refer to the artifacts of the current build directly:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>Note that if several tasks are uploading artifacts with the same name then the ZIP archive from the above link will contain merged content of all artifacts. It's also possible to refer to an artifact of a particular task within a build by name:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>It's also possible to download a particular file of an artifact and not the whole archive by using <code>&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;</code> instead of <code>&lt;ARTIFACTS_NAME&gt;.zip</code>.</p>","title":"Current build artifacts"},{"location":"guide/writing-tasks/#artifact-type","text":"<p>By default, Cirrus CI will try to guess mimetype of files in artifacts by looking at their extensions. In case when artifacts don't have extensions, it's possible to explicitly set the <code>Content-Type</code> via <code>type</code> field:</p> <pre><code>  my_task:\n    my_dotjar_artifacts:\n      path: build/*.jar\n      type: application/java-archive\n</code></pre> <p>A list of some of the basic types supported can be found here.</p>","title":"Artifact Type"},{"location":"guide/writing-tasks/#artifact-parsing","text":"<p>Cirrus CI supports parsing artifacts in order to extract information that can be presented in the UI for a better user experience. Use the <code>format</code> field of an artifact instruction to specify artifact's format (mimetypes):</p> <pre><code>junit_artifacts:\n  path: \"**/test-results/**.xml\"\n  type: text/xml\n  format: junit\n</code></pre> <p>Currently, Cirrus CI supports:</p> <ul> <li>Android Lint Report format</li> <li>GolangCI Lint's JSON format</li> <li>JUnit's XML format<ul> <li>Python's Unittest format</li> </ul> </li> <li>XCLogParser</li> <li>JetBrains Qodana</li> </ul> <p>Please let us know what kind of formats Cirrus CI should support next!</p>","title":"Artifact Parsing"},{"location":"guide/writing-tasks/#file-instruction","text":"<p>A <code>file</code> instruction allows to create a file from an environment variable. It is especially useful for situations when execution environment doesn't have proper shell to use <code>echo ... &gt;&gt; ...</code> syntax, for example, within scratch Docker containers.</p> <p>Here is an example of how to populate Docker config from an encrypted environment variable:</p> <pre><code>task:\n  environment:\n    DOCKER_CONFIG: ENCRYPTED[qwerty]\n  docker_config_file:\n    path: /root/.docker/config\n    variable_name: DOCKER_CONFIG\n</code></pre>","title":"File Instruction"},{"location":"guide/writing-tasks/#execution-behavior-of-instructions","text":"<p>By default Cirrus CI executes instructions one after another and stops the overall task execution on the first failure. Sometimes there might be situations when some scripts should always be executed or some debug information needs to be saved on a failure. For such situations the <code>always</code> and <code>on_failure</code> keywords can be used to group instructions.</p> <pre><code>task:\n  test_script: ./run_tests.sh\n  on_failure:\n    debug_script: ./print_additional_debug_info.sh\n  always:\n    test_reports_script: ./print_test_reports.sh\n</code></pre> <p>In the example above, <code>print_additional_debug_info.sh</code> script will be executed only on failures to output some additional debug information. <code>print_test_reports.sh</code> on the other hand will be executed both on successful and and failed runs to print test reports (test reports are always useful! ).</p>","title":"Execution Behavior of Instructions"},{"location":"guide/writing-tasks/#environment-variables","text":"<p>Environment variables can be configured under the <code>env</code> or <code>environment</code> keywords in <code>.cirrus.yml</code> files. Here is an example:</p> <pre><code>echo_task:\n  env:\n    FOO: Bar\n  echo_script: echo $FOO\n</code></pre> <p>You can reference other environment variables using <code>$VAR</code>, <code>${VAR}</code> or <code>%VAR%</code> syntax:</p> <pre><code>custom_path_task:\n  env:\n    SDK_ROOT: ${HOME}/sdk\n    PATH: ${SDK_ROOT}/bin:${PATH}\n  custom_script: sdktool install\n</code></pre> <p>Environment variables may also be set at the root level of <code>.cirrus.yml</code>. In that case, they will be merged with each task's individual environment variables, but the task level variables always take precedence. For example:</p> <pre><code>env:\n  PATH: /sdk/bin:${PATH}\n\necho_task:\n  env:\n    PATH: /opt/bin:${PATH}\n  echo_script: echo $PATH\n</code></pre> <p>Will output <code>/opt/bin:/usr/local/bin:/usr/bin</code> or similar, but will not include <code>/sdk/bin</code> because this root level setting is ignored.</p> <p>Also some default environment variables are pre-defined:</p>    Name Value / Description     CI true   CIRRUS_CI true   CI_NODE_INDEX Index of the current task within <code>CI_NODE_TOTAL</code> tasks   CI_NODE_TOTAL Total amount of unique tasks for a given <code>CIRRUS_BUILD_ID</code> build   CONTINUOUS_INTEGRATION <code>true</code>   CIRRUS_API_CREATED <code>true</code> if the current build was created through the API.   CIRRUS_BASE_BRANCH Base branch name if current build was triggered by a PR. For example <code>master</code>   CIRRUS_BASE_SHA Base SHA if current build was triggered by a PR   CIRRUS_BRANCH Branch name. For example <code>my-feature</code>   CIRRUS_BUILD_ID Unique build ID   CIRRUS_CHANGE_IN_REPO Git SHA   CIRRUS_CHANGE_MESSAGE Commit message or PR title and description, depending on trigger event (Non-PRs or PRs respectively).   CIRRUS_CHANGE_TITLE First line of <code>CIRRUS_CHANGE_MESSAGE</code>   CIRRUS_CRON Cron Build name if builds was triggered by Cron.   CIRRUS_DEFAULT_BRANCH Default repository branch name. For example <code>master</code>   CIRRUS_DOCKER_CONTEXT Docker build's context directory to use for Dockerfile as a CI environment. Defaults to project's root directory.   CIRRUS_LAST_GREEN_BUILD_ID The build id of the last successful build on the same branch at the time of the current build creation.   CIRRUS_LAST_GREEN_CHANGE Corresponding to <code>CIRRUS_LAST_GREEN_BUILD_ID</code> SHA (used in <code>changesInclude</code> and <code>changesIncludeOnly</code> functions).   CIRRUS_PR PR number if current build was triggered by a PR. For example <code>239</code>.   CIRRUS_PR_DRAFT <code>true</code> if current build was triggered by a Draft PR.   CIRRUS_TAG Tag name if current build was triggered by a new tag. For example <code>v1.0</code>   CIRRUS_OS, OS Host OS. Either <code>linux</code>, <code>windows</code> or <code>darwin</code>.   CIRRUS_TASK_NAME Task name   CIRRUS_TASK_ID Unique task ID   CIRRUS_RELEASE GitHub Release id if current tag was created for a release. Handy for uploading release assets.   CIRRUS_REPO_CLONE_TOKEN Temporary GitHub access token to perform a clone.   CIRRUS_REPO_NAME Repository name. For example <code>my-project</code>   CIRRUS_REPO_OWNER Repository owner (an organization or a user). For example <code>my-organization</code>   CIRRUS_REPO_FULL_NAME Repository full name/slug. For example <code>my-organization/my-project</code>   CIRRUS_REPO_CLONE_URL URL used for cloning. For example <code>https://github.com/my-organization/my-project.git</code>   CIRRUS_USER_COLLABORATOR <code>true</code> if a user initialized a build is already a contributor to the repository. <code>false</code> otherwise.   CIRRUS_USER_PERMISSION <code>admin</code>, <code>write</code>, <code>read</code> or <code>none</code>.   CIRRUS_HTTP_CACHE_HOST Host and port number on which local HTTP cache can be accessed on.   GITHUB_CHECK_SUITE_ID Monotonically increasing id of a corresponding GitHub Check Suite which caused the Cirrus CI build.   CIRRUS_ENV Path to a file, by writing to which you can set task-wide environment variables.","title":"Environment Variables"},{"location":"guide/writing-tasks/#behavioral-environment-variables","text":"<p>And some environment variables can be set to control behavior of the Cirrus CI Agent:</p>    Name Default Value Description     CIRRUS_CLONE_DEPTH <code>0</code> which will reflect in a full clone of a single branch Clone depth.   CIRRUS_CLONE_SUBMODULES <code>false</code> Set to <code>true</code> to clone submodules recursively.   CIRRUS_LOG_TIMESTAMP <code>false</code> Indicate Cirrus Agent to prepend timestamp to each line of logs.   CIRRUS_SHELL <code>sh</code> on Linux/macOS/FreeBSD and <code>cmd.exe</code> on Windows. Set to <code>direct</code> to execute each script directly without wrapping the commands in a shell script. Shell that Cirrus CI uses to execute scripts. By default <code>sh</code> is used.   CIRRUS_VOLUME <code>/tmp</code> Defines a path for a temporary volume to be mounted into instances running in a Kubernetes cluster. This volume is mounted into all additional containers and is persisted between steps of a <code>pipe</code>.   CIRRUS_WORKING_DIR <code>cirrus-ci-build</code> folder inside of a system's temporary folder Working directory where Cirrus CI executes builds. Default to <code>cirrus-ci-build</code> folder inside of a system's temporary folder.","title":"Behavioral Environment Variables"},{"location":"guide/writing-tasks/#encrypted-variables","text":"<p>It is possible to add encrypted variables to a <code>.cirrus.yml</code> file. These variables are decrypted only in builds for commits and pull requests that are made by users with <code>write</code> permission or approved by them.</p> <p>In order to encrypt a variable go to repository's settings page via clicking settings icon  on a repository's main page (for example <code>https://cirrus-ci.com/github/my-organization/my-repository</code>) and follow instructions.</p>  <p>Warning</p> <p>Only users with <code>WRITE</code> permissions can add encrypted variables to a repository.</p>  <p>An encrypted variable will be presented in a form like <code>ENCRYPTED[qwerty239abc]</code> which can be safely committed to <code>.cirrus.yml</code> file:</p> <pre><code>publish_task:\n  environment:\n    AUTH_TOKEN: ENCRYPTED[qwerty239abc]\n  script: ./publish.sh\n</code></pre> <p>Cirrus CI encrypts variables with a unique per repository 256-bit encryption key so forks and even repositories within the same organization cannot re-use them. <code>qwerty239abc</code> from the example above is NOT the content of your encrypted variable, it's just an internal ID. No one can brute force your secrets from such ID. In addition, Cirrus CI doesn't know a relation between an encrypted variable and a repository for which the encrypted variable was created.</p>  Organization Level Encrypted Variables <p>Sometimes there might be secrets that are used in almost all repositories of an organization. For example, credentials to a compute service where tasks will be executed. In order to create such sharable encrypted variable go to organization's settings page via clicking settings icon  on an organization's main page (for example <code>https://cirrus-ci.com/github/my-organization</code>) and follow instructions in Organization Level Encrypted Variables section.</p>   Encrypted Variable for Cloud Credentials <p>In case you use integration with one of supported computing services, an encrypted variable used to store credentials that Cirrus is using to communicate with the computing service won't be decrypted if used in environment variables. These credentials have too many permissions for most of the cases, please create separate credentials with the minimum needed permissions for your specific case.</p> <pre><code>gcp_credentials: SECURED[!qwerty]\n\nenv:\n  CREDENTIALS: SECURED[!qwerty] # won't be decrypted in any case\n</code></pre>   Skipping Task in Forked Repository <p>In forked repository the decryption of variable fails, which causes failure of task depending on it. To avoid this by default, make the sensitive task conditional:</p> <pre><code>task:\n  name: Task requiring decrypted variables\n  only_if: $CIRRUS_REPO_OWNER == 'my-organization'\n  ...\n</code></pre> <p>Owner of forked repository can re-enable the task, if they have the required sensitive data, by encrypting the variable by themselves and editing both the encrypted variable and repo-owner condition in the <code>.cirrus.yml</code> file.</p>","title":"Encrypted Variables"},{"location":"guide/writing-tasks/#cron-builds","text":"<p>It is possible to configure invocations of re-occurring builds via the well-known Cron expressions. Cron builds can be configured on a repository's settings page (not in <code>.cirrus.yml</code>).</p> <p>It's possible to configure several cron builds with unique <code>names</code> which will be available via <code>CIRRUS_CRON</code> environment variable. Each cron build should specify branch to trigger new builds for and a cron expression compatible with Quartz. You can use this generator to generate/validate your expressions.</p> <p>Note: Cron Builds are timed with the UTC timezone.</p>","title":"Cron Builds"},{"location":"guide/writing-tasks/#matrix-modification","text":"<p>Sometimes it's useful to run the same task against different software versions. Or run different batches of tests based on an environment variable. For cases like these, the <code>matrix</code> modifier comes very handy. It's possible to use <code>matrix</code> keyword only inside of a particular task to have multiple tasks based on the original one. Each new task will be created from the original task by replacing the whole <code>matrix</code> YAML node with each <code>matrix</code>'s children separately.</p> <p>Let check an example of a <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>test_task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre>    <p>Which will be expanded into:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  arm_container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre>     <p>Tip</p> <p>The <code>matrix</code> modifier can be used multiple times within a task.</p>  <p>The <code>matrix</code> modification makes it easy to create some pretty complex testing scenarios like this:</p> amd64arm64   <pre><code>task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre>   <pre><code>task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre>","title":"Matrix Modification"},{"location":"guide/writing-tasks/#task-execution-dependencies","text":"<p>Sometimes it might be very handy to execute some tasks only after successful execution of other tasks. For such cases it is possible to specify task names that a particular task depends. Use <code>depends_on</code> keyword to define dependencies:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre>     Task Names and Aliases <p>It is possible to specify the task's name via the <code>name</code> field. <code>lint_task</code> syntax is a syntactic sugar that will be expanded into:</p> <pre><code>task:\n  name: lint\n  ...\n</code></pre> <p>Names can be also pretty complex:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on:\n    - Test Shard 1/3\n    - Test Shard 2/3\n    - Test Shard 3/3\n  script: ./.ci/deploy.sh\n  ...\n</code></pre> <p>Complex task names make it difficult to list and maintain all of such task names in your <code>depends_on</code> field. In order to  make it simpler you can use the <code>alias</code> field to have a short simplified name for several tasks to use in <code>depends_on</code>.</p> <p>Here is a modified version of an example above that leverages the <code>alias</code> field:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  alias: Tests\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on: Tests\n  script: ./.ci/deploy.sh\n</code></pre>","title":"Task Execution Dependencies"},{"location":"guide/writing-tasks/#conditional-task-execution","text":"<p>Some tasks are meant to be created only if a certain condition is met. And some tasks can be skipped in some cases. Cirrus CI supports the <code>only_if</code> and <code>skip</code> keywords in order to provide such flexibility:</p>   <ul> <li> <p>The <code>only_if</code> keyword controls whether or not a task will be created. For example, you may want to publish only changes   committed to the <code>master</code> branch.   <pre><code>publish_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  script: yarn run publish\n</code></pre></p> </li> <li> <p>The <code>skip</code> keyword allows to skip execution of a task and mark it as successful. For example, you may want to skip linting   if no source files have changed since the last successful run.   <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre></p> </li> </ul>    <p>Skip CI Completely</p> <p>Just include <code>[skip ci]</code> or <code>[skip cirrus]</code> in the first line of your commit message in order to skip CI execution for a commit completely.</p> <p>If you push multiple commits at the same time, only the first line of the last commit message will be checked for <code>[skip ci]</code> or <code>[ci skip]</code>.</p> <p>If you open a PR, PR title will be checked for <code>[skip ci]</code> or <code>[ci skip]</code> instead of the last commit message on the PR branch.</p>","title":"Conditional Task Execution"},{"location":"guide/writing-tasks/#supported-operators","text":"<p>Currently only basic operators like <code>==</code>, <code>!=</code>, <code>=~</code>, <code>!=~</code>, <code>&amp;&amp;</code>, <code>||</code> and unary <code>!</code> are supported in <code>only_if</code> and <code>skip</code> expressions. Environment variables can also be used as usually.</p>  <p>Pattern Matching Example</p> <p>Use <code>=~</code> operator for pattern matching.</p> <pre><code>check_aggreement_task:\n  only_if: $CIRRUS_BRANCH =~ 'pull/.*'\n</code></pre> <p>Note that <code>=~</code> operator can match against multiline values (dotall mode) and therefore looking for the exact occurrence of the regular expression so don't forget to use <code>.*</code> around your term for matching it at any position (for example, <code>$CIRRUS_CHANGE_TITLE =~ '.*[docs].*'</code>).</p>","title":"Supported Operators"},{"location":"guide/writing-tasks/#supported-functions","text":"<p>Currently two functions are supported in the <code>only_if</code> and <code>skip</code> expressions:</p> <ul> <li><code>changesInclude</code> function allows to check which files were changed</li> <li><code>changesIncludeOnly</code> is a more strict version of <code>changesInclude</code>, i.e. it won't evaluate to <code>true</code> if there are changed files other than the ones covered by patterns</li> </ul> <p>These two functions behave differently for PR builds and regular builds:</p> <ul> <li>For PR builds, functions check the list of files affected by the PR.</li> <li>For regular builds, the <code>CIRRUS_LAST_GREEN_CHANGE</code> environment variable   will be used to determine list of affected files between <code>CIRRUS_LAST_GREEN_CHANGE</code> and <code>CIRRUS_CHANGE_IN_REPO</code>.</li> </ul> <p><code>changesInclude</code> function can be very useful for skipping some tasks when no changes to sources have been made since the last successful Cirrus CI build.</p> <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre> <p><code>changesIncludeOnly</code> function can be used to skip running a heavyweight task if only documentation was changed, for example:</p> <pre><code>build_task:\n  skip: \"changesIncludeOnly('doc/*')\"\n</code></pre>","title":"Supported Functions"},{"location":"guide/writing-tasks/#auto-cancellation-of-tasks","text":"<p>Cirrus CI can automatically cancel tasks in case of new pushes to the same branch. By default, Cirrus CI auto-cancels all tasks for non default branch (for most repositories <code>master</code> branch) but this behavior can be changed by specifying <code>auto_cancellation</code> field:</p> <pre><code>task:\n  auto_cancellation: $CIRRUS_BRANCH != 'master' &amp;&amp; $CIRRUS_BRANCH !=~ 'release/.*'\n  ...\n</code></pre>","title":"Auto-Cancellation of Tasks"},{"location":"guide/writing-tasks/#stateful-tasks","text":"<p>It's possible to tell Cirrus CI that a certain task is stateful and Cirrus CI will use a slightly different scheduling algorithm to minimize chances of such tasks being interrupted. Stateful tasks are intended to use low CPU count. Scheduling times of such stateful tasks might be a bit longer then usual especially for tasks with high CPU requirements.</p> <p>By default, Cirrus CI marks a task as stateful if its name contains one of the following terms: <code>deploy</code>, <code>push</code>, <code>publish</code>,  <code>upload</code> or <code>release</code>. Otherwise, you can explicitly mark a task as stateful via <code>stateful</code> field:</p> <pre><code>task:\n  name: Propagate to Production\n  stateful: true\n  ...\n</code></pre>","title":"Stateful Tasks"},{"location":"guide/writing-tasks/#failure-toleration","text":"<p>Sometimes tasks can play a role of sanity checks. For example, a task can check that your library is working with the latest nightly version of some dependency package. It will be great to be notified about such failures but it's not necessary to fail the whole build when a failure occurs. Cirrus CI has the <code>allow_failures</code> keyword which will make a task to not affect the overall status of a build.</p> <pre><code>test_nightly_task:\n  allow_failures: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre>  <p>Skipping Notifications</p> <p>You can also skip posting red statuses to GitHub via <code>skip_notifications</code> field.</p> <pre><code>skip_notifications: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre> <p>It can help to track potential issues overtime without distracting the main workflow.</p>","title":"Failure Toleration"},{"location":"guide/writing-tasks/#manual-tasks","text":"<p>By default a Cirrus CI task is automatically triggered when all its dependency tasks finished successfully. Sometimes though, it can be very handy to trigger some tasks manually, for example, perform a deployment to staging for manual testing upon all automation checks have succeeded. In order change the default behavior please use <code>trigger_type</code> field like this:</p> <pre><code>task:\n  name: \"Staging Deploy\"\n  trigger_type: manual\n  depends_on:\n    - Tests (Unit)\n    - Tests (Ingegration)\n    - Lint\n</code></pre> <p>You'll be able to manually trigger such paused tasks via Cirrus CI Web UI or directly from GitHub Checks page.</p>","title":"Manual tasks"},{"location":"guide/writing-tasks/#task-execution-lock","text":"<p>Some CI tasks perform external operations which are required to be executed one at a time. For example, parallel deploys to the same environment is usually a bad idea. In order to restrict parallel execution of a certain task within a repository, you can use <code>execution_lock</code> to specify a task's lock key, a unique string that will be used to make sure that any tasks with the same <code>execution_lock</code> string are executed one at a time. Here is an example of how to make sure deployments  on a specific branch can not run in parallel:</p> <pre><code>task:\n  name: \"Automatic Staging Deploy\"\n  execution_lock: $CIRRUS_BRANCH\n</code></pre> <p>You'll be able to manually trigger such paused tasks via the Cirrus CI Web Dashboard or directly from the commit's <code>checks</code> page on GitHub.</p>","title":"Task Execution Lock"},{"location":"guide/writing-tasks/#required-pr-labels","text":"<p>Similar to manual tasks Cirrus CI can pause execution of tasks until a corresponding PR gets labeled. This can be particular useful when you'd like to do an initial review before running all unit and integration tests on every supported platform. Use the <code>required_pr_labels</code> field to specify a list of labels a PR requires to have in order to trigger a task. Here is a simple example of <code>.cirrus.yml</code> config that automatically runs a linting tool but requires <code>initial-review</code> label being presented in order to run tests:</p> <pre><code>lint_task:\n  # ...\n\ntest_task:\n  required_pr_labels: initial-review\n  # ...\n</code></pre> <p>Note: <code>required_pr_labels</code> has no effect on tasks created for non-PR builds.</p> <p>You can also require multiple labels to continue executing the task for even more flexibility:</p> <pre><code>deploy_task:\n  required_pr_labels: \n    - initial-review\n    - ready-for-staging\n  depends_on: build\n  # ...\n</code></pre> <p>In the example above both <code>initial-review</code> and <code>ready-for-staging</code> labels should be presented on a PR in order to perform a deployment via <code>deploy</code> task.</p>","title":"Required PR Labels"},{"location":"guide/writing-tasks/#http-cache","text":"<p>For the most cases regular caching mechanism where Cirrus CI caches a folder is more than enough. But modern build systems like Gradle, Bazel and Pants can take advantage of remote caching. Remote caching is when a build system uploads and downloads intermediate results of a build execution while the build itself is still executing.</p> <p>Cirrus CI agent starts a local caching server and exposes it via <code>CIRRUS_HTTP_CACHE_HOST</code> environments variable. Caching server supports <code>GET</code>, <code>POST</code> and <code>HEAD</code> requests to upload, download and check presence of artifacts.</p>  <p>Info</p> <p>If port <code>12321</code> is available <code>CIRRUS_HTTP_CACHE_HOST</code> will be equal to <code>localhost:12321</code>.</p>  <p>For example running the following command:</p> <pre><code>curl -s -X POST --data-binary @myfolder.tar.gz http://$CIRRUS_HTTP_CACHE_HOST/name-key\n</code></pre> <p>...has the same effect as the following caching instruction:</p> <pre><code>name_cache:\n  folder: myfolder\n  fingerprint_key: key\n</code></pre>  <p>Info</p> <p>To see how HTTP Cache can be used with Gradle's Build Cache please check this example.</p>","title":"HTTP Cache"},{"location":"guide/writing-tasks/#additional-containers","text":"<p>Sometimes one container is not enough to run a CI build. For example, your application might use a MySQL database as a storage. In this case you most likely want a MySQL instance running for your tests.</p> <p>One option here is to pre-install MySQL and use a <code>background_script</code> to start it. This approach has some inconveniences like the need to pre-install MySQL by building a custom Docker container.</p> <p>For such use cases Cirrus CI allows to run additional containers in parallel with the main container that executes a task. Each additional container is defined under <code>additional_containers</code> keyword in <code>.cirrus.yml</code>. Each additional container should have a unique <code>name</code> and specify at least Docker <code>image</code> and <code>port</code> that this container exposes.</p> <p>In the example below we use an official MySQL Docker image that exposes the standard MySQL port (3306). Tests will be able to access MySQL instance via <code>localhost:3306</code>.</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>    <p>Additional container can be very handy in many scenarios. Please check Cirrus CI catalog of examples for more details.</p>  Default Resources <p>By default, each additional container will get <code>0.5</code> CPU and <code>512Mi</code> of memory. These values can be configured as usual via <code>cpu</code> and <code>memory</code> fields.</p>   Port Mapping <p>It's also possible to map ports of additional containers by using <code>&lt;HOST_PORT&gt;:&lt;CONTAINER_PORT&gt;</code> format for the <code>port</code> field. For example, <code>port: 80:8080</code> will map port <code>8080</code> of the container to be available on local port <code>80</code> within a task.</p> <p>Note: don't use port mapping unless absolutely necessary. A perfect use case is when you have several additional containers which start the service on the same port and there's no easy way to change that. Port mapping limits  the number of places the container can be scheduled and will affect how fast such tasks are scheduled.</p> <p>To specify multiple mappings use the <code>ports</code> field, instead of the <code>port</code>: <pre><code>ports:\n  - 8080\n  - 3306\n</code></pre></p>   Overriding Default Command <p>It's also possible to override the default <code>CMD</code> of an additional container via <code>command</code> field:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n</code></pre>      Warning <p>Note that <code>additional_containers</code> can be used only with Community Cluster or Google's Kubernetes Engine.</p>","title":"Additional Containers"},{"location":"guide/writing-tasks/#embedded-badges","text":"<p>Cirrus CI provides a way to embed a badge that can represent status of your builds into a ReadMe file or a website.</p> <p>For example, this is a badge for <code>cirruslabs/cirrus-ci-web</code> repository that contains Cirrus CI's front end: </p> <p>In order to embed such a check into a \"read-me\" file or your website, just use a URL to a badge that looks like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg\n</code></pre> <p>If you want a badge for a particular branch, use the <code>?branch=&lt;BRANCH NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?branch=&lt;BRANCH NAME&gt;\n</code></pre> <p>By default, Cirrus picks the latest build in a final state for the repository or a particular branch if <code>branch</code> parameter is specified. It's also possible to explicitly set a concrete build to use with <code>?buildId=&lt;BUILD ID&gt;</code> query parameter.</p> <p>If you want a badge for a particular task within the latest finished build, use the <code>?task=&lt;TASK NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=tests\n</code></pre> <p>You can even pick a specific script instruction within the task with an additional <code>script=&lt;SCRIPT NAME&gt;</code> parameter:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=build&amp;script=lint\n</code></pre>","title":"Embedded Badges"},{"location":"guide/writing-tasks/#badges-in-markdown","text":"<p>Here is how Cirrus CI's badge can be embeded in a Markdown file:</p> <pre><code>[![Build Status](https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg)](https://cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;)\n</code></pre>","title":"Badges in Markdown"},{"location":"guide/writing-tasks/#cctray-xml","text":"<p>Cirrus CI supports exporting information about the latest repository builds via the CCTray XML format. Use the following URL format with a tool of your choice (such as CCMenu):</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/cctray.xml\n</code></pre> <p>Note: for private repositories you'll need to configure access token.</p>","title":"CCTray XML"}]})